PS C:\mygit\MultiModalClassifier> python .\TorchClassifier\myTorchTrainer.py --data_name 'CIFAR10' --data_type 'torchvisiondataset' --data_path "C:\mygit\TrainingData\data\" --model_name 'resnetmodel1' --learningratename 'ConstantLR' --optimizer 'SGD'
2.2.1
Torch Version:  2.2.1
Torchvision Version:  0.17.1
Output path: ./outputs/CIFAR10_resnetmodel1_0910
Num GPUs: 1
0
NVIDIA GeForce RTX 3070 Ti
True
Files already downloaded and verified
Files already downloaded and verified
Number of training examples: 50000
Number of testing examples: 10000
========================================================================================================================
Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable
========================================================================================================================
ResNet (ResNet)                          [128, 3, 224, 224]   [128, 10]            --                   True
├─Conv2d (conv1)                         [128, 3, 224, 224]   [128, 64, 112, 112]  9,408                True
├─BatchNorm2d (bn1)                      [128, 64, 112, 112]  [128, 64, 112, 112]  128                  True
├─ReLU (relu)                            [128, 64, 112, 112]  [128, 64, 112, 112]  --                   --
├─MaxPool2d (maxpool)                    [128, 64, 112, 112]  [128, 64, 56, 56]    --                   --
├─Sequential (layer1)                    [128, 64, 56, 56]    [128, 64, 56, 56]    --                   True
│    └─BasicBlock (0)                    [128, 64, 56, 56]    [128, 64, 56, 56]    --                   True
│    │    └─Conv2d (conv1)               [128, 64, 56, 56]    [128, 64, 56, 56]    36,864               True
│    │    └─BatchNorm2d (bn1)            [128, 64, 56, 56]    [128, 64, 56, 56]    128                  True
│    │    └─ReLU (relu)                  [128, 64, 56, 56]    [128, 64, 56, 56]    --                   --
│    │    └─Conv2d (conv2)               [128, 64, 56, 56]    [128, 64, 56, 56]    36,864               True
│    │    └─BatchNorm2d (bn2)            [128, 64, 56, 56]    [128, 64, 56, 56]    128                  True
│    │    └─ReLU (relu)                  [128, 64, 56, 56]    [128, 64, 56, 56]    --                   --
│    └─BasicBlock (1)                    [128, 64, 56, 56]    [128, 64, 56, 56]    --                   True
│    │    └─Conv2d (conv1)               [128, 64, 56, 56]    [128, 64, 56, 56]    36,864               True
│    │    └─BatchNorm2d (bn1)            [128, 64, 56, 56]    [128, 64, 56, 56]    128                  True
│    │    └─ReLU (relu)                  [128, 64, 56, 56]    [128, 64, 56, 56]    --                   --
│    │    └─Conv2d (conv2)               [128, 64, 56, 56]    [128, 64, 56, 56]    36,864               True
│    │    └─BatchNorm2d (bn2)            [128, 64, 56, 56]    [128, 64, 56, 56]    128                  True
│    │    └─ReLU (relu)                  [128, 64, 56, 56]    [128, 64, 56, 56]    --                   --
├─Sequential (layer2)                    [128, 64, 56, 56]    [128, 128, 28, 28]   --                   True
│    └─BasicBlock (0)                    [128, 64, 56, 56]    [128, 128, 28, 28]   --                   True
│    │    └─Conv2d (conv1)               [128, 64, 56, 56]    [128, 128, 28, 28]   73,728               True
│    │    └─BatchNorm2d (bn1)            [128, 128, 28, 28]   [128, 128, 28, 28]   256                  True
│    │    └─ReLU (relu)                  [128, 128, 28, 28]   [128, 128, 28, 28]   --                   --
│    │    └─Conv2d (conv2)               [128, 128, 28, 28]   [128, 128, 28, 28]   147,456              True
│    │    └─BatchNorm2d (bn2)            [128, 128, 28, 28]   [128, 128, 28, 28]   256                  True
│    │    └─Sequential (downsample)      [128, 64, 56, 56]    [128, 128, 28, 28]   8,448                True
│    │    └─ReLU (relu)                  [128, 128, 28, 28]   [128, 128, 28, 28]   --                   --
│    └─BasicBlock (1)                    [128, 128, 28, 28]   [128, 128, 28, 28]   --                   True
│    │    └─Conv2d (conv1)               [128, 128, 28, 28]   [128, 128, 28, 28]   147,456              True
│    │    └─BatchNorm2d (bn1)            [128, 128, 28, 28]   [128, 128, 28, 28]   256                  True
│    │    └─ReLU (relu)                  [128, 128, 28, 28]   [128, 128, 28, 28]   --                   --
│    │    └─Conv2d (conv2)               [128, 128, 28, 28]   [128, 128, 28, 28]   147,456              True
│    │    └─BatchNorm2d (bn2)            [128, 128, 28, 28]   [128, 128, 28, 28]   256                  True
│    │    └─ReLU (relu)                  [128, 128, 28, 28]   [128, 128, 28, 28]   --                   --
├─Sequential (layer3)                    [128, 128, 28, 28]   [128, 256, 14, 14]   --                   True
│    └─BasicBlock (0)                    [128, 128, 28, 28]   [128, 256, 14, 14]   --                   True
│    │    └─Conv2d (conv1)               [128, 128, 28, 28]   [128, 256, 14, 14]   294,912              True
│    │    └─BatchNorm2d (bn1)            [128, 256, 14, 14]   [128, 256, 14, 14]   512                  True
│    │    └─ReLU (relu)                  [128, 256, 14, 14]   [128, 256, 14, 14]   --                   --
│    │    └─Conv2d (conv2)               [128, 256, 14, 14]   [128, 256, 14, 14]   589,824              True
│    │    └─BatchNorm2d (bn2)            [128, 256, 14, 14]   [128, 256, 14, 14]   512                  True
│    │    └─Sequential (downsample)      [128, 128, 28, 28]   [128, 256, 14, 14]   33,280               True
│    │    └─ReLU (relu)                  [128, 256, 14, 14]   [128, 256, 14, 14]   --                   --
│    └─BasicBlock (1)                    [128, 256, 14, 14]   [128, 256, 14, 14]   --                   True
│    │    └─Conv2d (conv1)               [128, 256, 14, 14]   [128, 256, 14, 14]   589,824              True
│    │    └─BatchNorm2d (bn1)            [128, 256, 14, 14]   [128, 256, 14, 14]   512                  True
│    │    └─ReLU (relu)                  [128, 256, 14, 14]   [128, 256, 14, 14]   --                   --
│    │    └─Conv2d (conv2)               [128, 256, 14, 14]   [128, 256, 14, 14]   589,824              True
│    │    └─BatchNorm2d (bn2)            [128, 256, 14, 14]   [128, 256, 14, 14]   512                  True
│    │    └─ReLU (relu)                  [128, 256, 14, 14]   [128, 256, 14, 14]   --                   --
├─Sequential (layer4)                    [128, 256, 14, 14]   [128, 512, 7, 7]     --                   True
│    └─BasicBlock (0)                    [128, 256, 14, 14]   [128, 512, 7, 7]     --                   True
│    │    └─Conv2d (conv1)               [128, 256, 14, 14]   [128, 512, 7, 7]     1,179,648            True
│    │    └─BatchNorm2d (bn1)            [128, 512, 7, 7]     [128, 512, 7, 7]     1,024                True
│    │    └─ReLU (relu)                  [128, 512, 7, 7]     [128, 512, 7, 7]     --                   --
│    │    └─Conv2d (conv2)               [128, 512, 7, 7]     [128, 512, 7, 7]     2,359,296            True
│    │    └─BatchNorm2d (bn2)            [128, 512, 7, 7]     [128, 512, 7, 7]     1,024                True
│    │    └─Sequential (downsample)      [128, 256, 14, 14]   [128, 512, 7, 7]     132,096              True
│    │    └─ReLU (relu)                  [128, 512, 7, 7]     [128, 512, 7, 7]     --                   --
│    └─BasicBlock (1)                    [128, 512, 7, 7]     [128, 512, 7, 7]     --                   True
│    │    └─Conv2d (conv1)               [128, 512, 7, 7]     [128, 512, 7, 7]     2,359,296            True
│    │    └─BatchNorm2d (bn1)            [128, 512, 7, 7]     [128, 512, 7, 7]     1,024                True
│    │    └─ReLU (relu)                  [128, 512, 7, 7]     [128, 512, 7, 7]     --                   --
│    │    └─Conv2d (conv2)               [128, 512, 7, 7]     [128, 512, 7, 7]     2,359,296            True
│    │    └─BatchNorm2d (bn2)            [128, 512, 7, 7]     [128, 512, 7, 7]     1,024                True
│    │    └─ReLU (relu)                  [128, 512, 7, 7]     [128, 512, 7, 7]     --                   --
├─AdaptiveAvgPool2d (avgpool)            [128, 512, 7, 7]     [128, 512, 1, 1]     --                   --
├─Linear (fc)                            [128, 512]           [128, 10]            5,130                True
========================================================================================================================
Total params: 11,181,642
Trainable params: 11,181,642
Non-trainable params: 0
Total mult-adds (Units.GIGABYTES): 232.14
========================================================================================================================
Input size (MB): 77.07
Forward/backward pass size (MB): 5086.65
Params size (MB): 44.73
Estimated Total Size (MB): 5208.45
========================================================================================================================
=> no checkpoint found at 'outputs/imagenet_blurred_resnet50_0328/model_best.pth.tar'
Epoch 0/39
----------
Epoch: [0][  1/313]     Time  0.510 ( 0.510)    Data  0.083 ( 0.083)    Loss 2.6386e+00 (2.6386e+00)    Acc@1  10.94 ( 10.94)   Acc@5  51.56 ( 51.56)
STAGE:2024-03-26 16:39:28 107796:108028 C:\cb\pytorch_1000000000000\work\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:314] Completed Stage: Warm Up
STAGE:2024-03-26 16:39:29 107796:108028 C:\cb\pytorch_1000000000000\work\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:320] Completed Stage: Collection
STAGE:2024-03-26 16:39:29 107796:108028 C:\cb\pytorch_1000000000000\work\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:324] Completed Stage: Post Processing
STAGE:2024-03-26 16:39:31 107796:108028 C:\cb\pytorch_1000000000000\work\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:314] Completed Stage: Warm Up
STAGE:2024-03-26 16:39:31 107796:108028 C:\cb\pytorch_1000000000000\work\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:320] Completed Stage: Collection
STAGE:2024-03-26 16:39:31 107796:108028 C:\cb\pytorch_1000000000000\work\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:324] Completed Stage: Post Processing
Epoch: [0][101/313]     Time  0.048 ( 0.092)    Data  0.031 ( 0.072)    Loss 1.9527e+00 (1.9561e+00)    Acc@1  46.09 ( 39.80)   Acc@5  92.19 ( 86.52)
Epoch: [0][201/313]     Time  0.040 ( 0.067)    Data  0.023 ( 0.049)    Loss 2.2983e+00 (1.8439e+00)    Acc@1  47.66 ( 43.56)   Acc@5  90.62 ( 89.02)
Epoch: [0][301/313]     Time  0.042 ( 0.059)    Data  0.024 ( 0.041)    Loss 1.6557e+00 (1.8408e+00)    Acc@1  40.62 ( 42.89)   Acc@5  93.75 ( 88.95)
train Loss: 1.8365 Acc: 0.4274
Epoch: [0][  1/313]     Time  0.045 ( 0.058)    Data  0.036 ( 0.040)    Loss 1.9567e+00 (1.8369e+00)    Acc@1  35.16 ( 42.72)   Acc@5  83.59 ( 88.92)
val Loss: 1.6677 Acc: 0.4250

Epoch 1/39
----------
Epoch: [1][  1/313]     Time  0.099 ( 0.099)    Data  0.076 ( 0.076)    Loss 1.5413e+00 (1.5413e+00)    Acc@1  39.06 ( 39.06)   Acc@5  96.09 ( 96.09)
Epoch: [1][101/313]     Time  0.047 ( 0.045)    Data  0.027 ( 0.027)    Loss 1.4433e+00 (1.5080e+00)    Acc@1  52.34 ( 47.28)   Acc@5  94.53 ( 92.56)
Epoch: [1][201/313]     Time  0.042 ( 0.049)    Data  0.025 ( 0.030)    Loss 1.3736e+00 (1.4259e+00)    Acc@1  49.22 ( 50.66)   Acc@5  95.31 ( 92.93)
Epoch: [1][301/313]     Time  0.042 ( 0.047)    Data  0.024 ( 0.028)    Loss 1.1499e+00 (1.3825e+00)    Acc@1  54.69 ( 51.91)   Acc@5  98.44 ( 93.33)
train Loss: 1.3751 Acc: 0.5217
Epoch: [1][  1/313]     Time  0.044 ( 0.047)    Data  0.036 ( 0.028)    Loss 1.1846e+00 (1.3745e+00)    Acc@1  53.91 ( 52.18)   Acc@5  92.97 ( 93.40)
val Loss: 1.1469 Acc: 0.5901

Epoch 2/39
----------
Epoch: [2][  1/313]     Time  0.098 ( 0.098)    Data  0.076 ( 0.076)    Loss 1.0800e+00 (1.0800e+00)    Acc@1  62.50 ( 62.50)   Acc@5  96.09 ( 96.09)
Epoch: [2][101/313]     Time  0.042 ( 0.043)    Data  0.024 ( 0.024)    Loss 1.0202e+00 (1.0903e+00)    Acc@1  64.84 ( 61.28)   Acc@5  98.44 ( 96.51)
Epoch: [2][201/313]     Time  0.041 ( 0.043)    Data  0.023 ( 0.024)    Loss 1.0240e+00 (1.0653e+00)    Acc@1  65.62 ( 62.69)   Acc@5  96.88 ( 96.56)
Epoch: [2][301/313]     Time  0.041 ( 0.042)    Data  0.024 ( 0.024)    Loss 1.0034e+00 (1.0330e+00)    Acc@1  66.41 ( 63.92)   Acc@5  96.09 ( 96.68)
train Loss: 1.0282 Acc: 0.6411
Epoch: [2][  1/313]     Time  0.045 ( 0.042)    Data  0.037 ( 0.024)    Loss 1.1010e+00 (1.0284e+00)    Acc@1  58.59 ( 64.09)   Acc@5  96.09 ( 96.67)
val Loss: 1.0122 Acc: 0.6574

Epoch 3/39
----------
Epoch: [3][  1/313]     Time  0.097 ( 0.097)    Data  0.073 ( 0.073)    Loss 9.0053e-01 (9.0053e-01)    Acc@1  68.75 ( 68.75)   Acc@5  97.66 ( 97.66)
Epoch: [3][101/313]     Time  0.043 ( 0.043)    Data  0.025 ( 0.024)    Loss 1.0216e+00 (8.5178e-01)    Acc@1  67.97 ( 70.03)   Acc@5  96.88 ( 97.90)
Epoch: [3][201/313]     Time  0.041 ( 0.043)    Data  0.024 ( 0.024)    Loss 8.8410e-01 (8.5101e-01)    Acc@1  72.66 ( 70.37)   Acc@5  97.66 ( 97.82)
Epoch: [3][301/313]     Time  0.043 ( 0.043)    Data  0.024 ( 0.025)    Loss 9.6586e-01 (8.5060e-01)    Acc@1  69.53 ( 70.44)   Acc@5  99.22 ( 97.73)
train Loss: 0.8507 Acc: 0.7040
Epoch: [3][  1/313]     Time  0.041 ( 0.043)    Data  0.034 ( 0.025)    Loss 1.0265e+00 (8.5128e-01)    Acc@1  61.72 ( 70.37)   Acc@5  96.88 ( 97.73)
val Loss: 0.8701 Acc: 0.6963

Epoch 4/39
----------
Epoch: [4][  1/313]     Time  0.110 ( 0.110)    Data  0.085 ( 0.085)    Loss 7.7725e-01 (7.7725e-01)    Acc@1  71.88 ( 71.88)   Acc@5  99.22 ( 99.22)
Epoch: [4][101/313]     Time  0.042 ( 0.050)    Data  0.025 ( 0.030)    Loss 9.5291e-01 (9.1221e-01)    Acc@1  70.31 ( 68.47)   Acc@5  99.22 ( 97.27)
Epoch: [4][201/313]     Time  0.042 ( 0.046)    Data  0.025 ( 0.027)    Loss 1.0225e+00 (9.1676e-01)    Acc@1  60.94 ( 68.49)   Acc@5  97.66 ( 97.23)
Epoch: [4][301/313]     Time  0.042 ( 0.046)    Data  0.024 ( 0.028)    Loss 7.9855e-01 (8.9468e-01)    Acc@1  74.22 ( 69.26)   Acc@5  99.22 ( 97.36)
train Loss: 0.8932 Acc: 0.6930
Epoch: [4][  1/313]     Time  0.044 ( 0.046)    Data  0.036 ( 0.028)    Loss 1.0332e+00 (8.9366e-01)    Acc@1  62.50 ( 69.28)   Acc@5  94.53 ( 97.35)
val Loss: 1.1027 Acc: 0.6351

Epoch 5/39
----------
Epoch: [5][  1/313]     Time  0.094 ( 0.094)    Data  0.073 ( 0.073)    Loss 6.8374e-01 (6.8374e-01)    Acc@1  76.56 ( 76.56)   Acc@5  99.22 ( 99.22)
Epoch: [5][101/313]     Time  0.068 ( 0.044)    Data  0.047 ( 0.025)    Loss 8.9533e-01 (7.5523e-01)    Acc@1  64.84 ( 74.24)   Acc@5  97.66 ( 97.97)
Epoch: [5][201/313]     Time  0.041 ( 0.044)    Data  0.024 ( 0.025)    Loss 8.0657e-01 (7.6900e-01)    Acc@1  71.88 ( 73.58)   Acc@5  98.44 ( 98.03)
Epoch: [5][301/313]     Time  0.041 ( 0.043)    Data  0.023 ( 0.025)    Loss 6.2974e-01 (7.5076e-01)    Acc@1  80.47 ( 74.23)   Acc@5  99.22 ( 98.11)
train Loss: 0.7490 Acc: 0.7425
Epoch: [5][  1/313]     Time  0.044 ( 0.043)    Data  0.035 ( 0.025)    Loss 8.5168e-01 (7.4935e-01)    Acc@1  70.31 ( 74.24)   Acc@5  97.66 ( 98.12)
val Loss: 0.8293 Acc: 0.7147

Epoch 6/39
----------
Epoch: [6][  1/313]     Time  0.099 ( 0.099)    Data  0.075 ( 0.075)    Loss 5.7492e-01 (5.7492e-01)    Acc@1  79.69 ( 79.69)   Acc@5  99.22 ( 99.22)
Epoch: [6][101/313]     Time  0.040 ( 0.045)    Data  0.024 ( 0.026)    Loss 6.9918e-01 (6.0184e-01)    Acc@1  77.34 ( 79.57)   Acc@5  97.66 ( 98.69)
Epoch: [6][201/313]     Time  0.049 ( 0.044)    Data  0.025 ( 0.025)    Loss 6.4113e-01 (6.0893e-01)    Acc@1  74.22 ( 79.28)   Acc@5 100.00 ( 98.75)
Epoch: [6][301/313]     Time  0.042 ( 0.044)    Data  0.024 ( 0.025)    Loss 6.7919e-01 (6.1778e-01)    Acc@1  76.56 ( 78.92)   Acc@5  99.22 ( 98.72)
train Loss: 0.6199 Acc: 0.7884
Epoch: [6][  1/313]     Time  0.045 ( 0.044)    Data  0.036 ( 0.025)    Loss 9.0801e-01 (6.2084e-01)    Acc@1  69.53 ( 78.81)   Acc@5  97.66 ( 98.71)
val Loss: 0.8197 Acc: 0.7213

Epoch 7/39
----------
Epoch: [7][  1/313]     Time  0.105 ( 0.105)    Data  0.076 ( 0.076)    Loss 5.3251e-01 (5.3251e-01)    Acc@1  82.81 ( 82.81)   Acc@5  98.44 ( 98.44)
Epoch: [7][101/313]     Time  0.042 ( 0.044)    Data  0.024 ( 0.025)    Loss 5.1156e-01 (5.1422e-01)    Acc@1  82.81 ( 82.34)   Acc@5  99.22 ( 99.28)
Epoch: [7][201/313]     Time  0.037 ( 0.044)    Data  0.024 ( 0.025)    Loss 4.6118e-01 (5.2726e-01)    Acc@1  84.38 ( 81.83)   Acc@5 100.00 ( 99.19)
Epoch: [7][301/313]     Time  0.041 ( 0.042)    Data  0.024 ( 0.025)    Loss 4.6178e-01 (5.3468e-01)    Acc@1  85.16 ( 81.55)   Acc@5  99.22 ( 99.14)
train Loss: 0.5334 Acc: 0.8156
Epoch: [7][  1/313]     Time  0.046 ( 0.042)    Data  0.038 ( 0.025)    Loss 7.9223e-01 (5.3421e-01)    Acc@1  74.22 ( 81.54)   Acc@5  96.88 ( 99.15)
val Loss: 0.7674 Acc: 0.7435

Epoch 8/39
----------
Epoch: [8][  1/313]     Time  0.093 ( 0.093)    Data  0.072 ( 0.072)    Loss 2.7777e-01 (2.7777e-01)    Acc@1  89.06 ( 89.06)   Acc@5 100.00 (100.00)
Epoch: [8][101/313]     Time  0.041 ( 0.044)    Data  0.024 ( 0.026)    Loss 3.6592e-01 (4.1669e-01)    Acc@1  88.28 ( 85.64)   Acc@5 100.00 ( 99.41)
Epoch: [8][201/313]     Time  0.040 ( 0.043)    Data  0.023 ( 0.025)    Loss 5.5348e-01 (4.5143e-01)    Acc@1  80.47 ( 84.49)   Acc@5  98.44 ( 99.28)
Epoch: [8][301/313]     Time  0.043 ( 0.042)    Data  0.025 ( 0.025)    Loss 3.8905e-01 (4.5695e-01)    Acc@1  85.94 ( 84.17)   Acc@5  99.22 ( 99.31)
train Loss: 0.4582 Acc: 0.8411
Epoch: [8][  1/313]     Time  0.043 ( 0.042)    Data  0.037 ( 0.025)    Loss 8.0231e-01 (4.5928e-01)    Acc@1  75.00 ( 84.08)   Acc@5  95.31 ( 99.31)
val Loss: 0.8121 Acc: 0.7414

Epoch 9/39
----------
Epoch: [9][  1/313]     Time  0.099 ( 0.099)    Data  0.085 ( 0.085)    Loss 3.1375e-01 (3.1375e-01)    Acc@1  86.72 ( 86.72)   Acc@5 100.00 (100.00)
Epoch: [9][101/313]     Time  0.043 ( 0.042)    Data  0.026 ( 0.026)    Loss 4.8254e-01 (3.5471e-01)    Acc@1  84.38 ( 87.87)   Acc@5  98.44 ( 99.62)
Epoch: [9][201/313]     Time  0.041 ( 0.043)    Data  0.025 ( 0.026)    Loss 4.6023e-01 (3.9836e-01)    Acc@1  83.59 ( 86.35)   Acc@5 100.00 ( 99.55)
Epoch: [9][301/313]     Time  0.045 ( 0.043)    Data  0.025 ( 0.026)    Loss 5.1908e-01 (4.0446e-01)    Acc@1  78.91 ( 85.99)   Acc@5  98.44 ( 99.54)
train Loss: 0.4072 Acc: 0.8586
Epoch: [9][  1/313]     Time  0.044 ( 0.043)    Data  0.036 ( 0.026)    Loss 9.8002e-01 (4.0906e-01)    Acc@1  67.97 ( 85.81)   Acc@5  97.66 ( 99.52)
val Loss: 0.7885 Acc: 0.7503

Epoch 10/39
----------
Epoch: [10][  1/313]    Time  0.098 ( 0.098)    Data  0.074 ( 0.074)    Loss 3.9565e-01 (3.9565e-01)    Acc@1  86.72 ( 86.72)   Acc@5 100.00 (100.00)
Epoch: [10][101/313]    Time  0.042 ( 0.043)    Data  0.024 ( 0.024)    Loss 2.8993e-01 (3.1462e-01)    Acc@1  89.84 ( 89.18)   Acc@5 100.00 ( 99.76)
Epoch: [10][201/313]    Time  0.041 ( 0.042)    Data  0.023 ( 0.024)    Loss 4.5691e-01 (3.2538e-01)    Acc@1  85.16 ( 88.77)   Acc@5  98.44 ( 99.72)
Epoch: [10][301/313]    Time  0.043 ( 0.042)    Data  0.026 ( 0.024)    Loss 3.6908e-01 (3.4200e-01)    Acc@1  86.72 ( 88.17)   Acc@5 100.00 ( 99.69)
train Loss: 0.3425 Acc: 0.8814
Epoch: [10][  1/313]    Time  0.045 ( 0.042)    Data  0.037 ( 0.024)    Loss 6.6722e-01 (3.4355e-01)    Acc@1  77.34 ( 88.10)   Acc@5  99.22 ( 99.69)
val Loss: 0.7916 Acc: 0.7556

Epoch 11/39
----------
Epoch: [11][  1/313]    Time  0.098 ( 0.098)    Data  0.072 ( 0.072)    Loss 2.3769e-01 (2.3769e-01)    Acc@1  93.75 ( 93.75)   Acc@5  99.22 ( 99.22)
Epoch: [11][101/313]    Time  0.041 ( 0.043)    Data  0.024 ( 0.025)    Loss 2.8072e-01 (2.6928e-01)    Acc@1  90.62 ( 90.81)   Acc@5 100.00 ( 99.82)
Epoch: [11][201/313]    Time  0.040 ( 0.042)    Data  0.023 ( 0.024)    Loss 2.5900e-01 (2.7873e-01)    Acc@1  91.41 ( 90.44)   Acc@5 100.00 ( 99.81)
Epoch: [11][301/313]    Time  0.040 ( 0.042)    Data  0.023 ( 0.024)    Loss 3.1827e-01 (2.9350e-01)    Acc@1  88.28 ( 89.86)   Acc@5 100.00 ( 99.79)
train Loss: 0.2942 Acc: 0.8983
Epoch: [11][  1/313]    Time  0.045 ( 0.042)    Data  0.038 ( 0.024)    Loss 1.0862e+00 (2.9675e-01)    Acc@1  71.09 ( 89.77)   Acc@5  99.22 ( 99.80)
val Loss: 1.0892 Acc: 0.7016

Epoch 12/39
----------
Epoch: [12][  1/313]    Time  0.105 ( 0.105)    Data  0.078 ( 0.078)    Loss 1.6063e-01 (1.6063e-01)    Acc@1  94.53 ( 94.53)   Acc@5 100.00 (100.00)
Epoch: [12][101/313]    Time  0.041 ( 0.043)    Data  0.024 ( 0.024)    Loss 1.9533e-01 (2.2702e-01)    Acc@1  92.97 ( 92.26)   Acc@5 100.00 ( 99.88)
Epoch: [12][201/313]    Time  0.042 ( 0.042)    Data  0.026 ( 0.024)    Loss 3.8083e-01 (2.4280e-01)    Acc@1  87.50 ( 91.65)   Acc@5  99.22 ( 99.86)
Epoch: [12][301/313]    Time  0.041 ( 0.042)    Data  0.024 ( 0.024)    Loss 2.2430e-01 (2.5773e-01)    Acc@1  91.41 ( 91.08)   Acc@5 100.00 ( 99.86)
train Loss: 0.2587 Acc: 0.9105
Epoch: [12][  1/313]    Time  0.043 ( 0.042)    Data  0.035 ( 0.024)    Loss 6.7242e-01 (2.5998e-01)    Acc@1  81.25 ( 91.02)   Acc@5  99.22 ( 99.85)
val Loss: 0.9052 Acc: 0.7467

Epoch 13/39
----------
Epoch: [13][  1/313]    Time  0.198 ( 0.198)    Data  0.175 ( 0.175)    Loss 1.4279e-01 (1.4279e-01)    Acc@1  96.09 ( 96.09)   Acc@5  99.22 ( 99.22)
Epoch: [13][101/313]    Time  0.044 ( 0.044)    Data  0.025 ( 0.026)    Loss 1.9350e-01 (1.9130e-01)    Acc@1  92.19 ( 93.29)   Acc@5  99.22 ( 99.90)
Epoch: [13][201/313]    Time  0.042 ( 0.043)    Data  0.024 ( 0.025)    Loss 3.9372e-01 (2.1439e-01)    Acc@1  87.50 ( 92.57)   Acc@5  99.22 ( 99.90)
Epoch: [13][301/313]    Time  0.042 ( 0.043)    Data  0.023 ( 0.025)    Loss 2.5341e-01 (2.2338e-01)    Acc@1  92.19 ( 92.25)   Acc@5 100.00 ( 99.89)
train Loss: 0.2236 Acc: 0.9224
Epoch: [13][  1/313]    Time  0.048 ( 0.043)    Data  0.040 ( 0.025)    Loss 1.1555e+00 (2.2655e-01)    Acc@1  72.66 ( 92.18)   Acc@5  98.44 ( 99.88)
val Loss: 0.8969 Acc: 0.7505

Epoch 14/39
----------
Epoch: [14][  1/313]    Time  0.097 ( 0.097)    Data  0.072 ( 0.072)    Loss 1.0852e-01 (1.0852e-01)    Acc@1  97.66 ( 97.66)   Acc@5 100.00 (100.00)
Epoch: [14][101/313]    Time  0.043 ( 0.044)    Data  0.025 ( 0.025)    Loss 2.4330e-01 (1.5379e-01)    Acc@1  89.84 ( 94.57)   Acc@5 100.00 ( 99.94)
Epoch: [14][201/313]    Time  0.042 ( 0.043)    Data  0.024 ( 0.024)    Loss 1.9354e-01 (1.8013e-01)    Acc@1  93.75 ( 93.60)   Acc@5 100.00 ( 99.93)
Epoch: [14][301/313]    Time  0.042 ( 0.042)    Data  0.024 ( 0.024)    Loss 2.7965e-01 (1.9259e-01)    Acc@1  89.06 ( 93.25)   Acc@5  99.22 ( 99.90)
train Loss: 0.1956 Acc: 0.9314
Epoch: [14][  1/313]    Time  0.045 ( 0.042)    Data  0.037 ( 0.024)    Loss 7.6870e-01 (1.9742e-01)    Acc@1  79.69 ( 93.10)   Acc@5  99.22 ( 99.90)
val Loss: 0.9549 Acc: 0.7464

Epoch 15/39
----------
Epoch: [15][  1/313]    Time  0.093 ( 0.093)    Data  0.079 ( 0.079)    Loss 1.2541e-01 (1.2541e-01)    Acc@1  95.31 ( 95.31)   Acc@5 100.00 (100.00)
Epoch: [15][101/313]    Time  0.038 ( 0.039)    Data  0.023 ( 0.025)    Loss 1.3984e-01 (1.4933e-01)    Acc@1  95.31 ( 94.96)   Acc@5 100.00 ( 99.98)
Epoch: [15][201/313]    Time  0.040 ( 0.040)    Data  0.023 ( 0.025)    Loss 1.4462e-01 (1.6828e-01)    Acc@1  93.75 ( 94.20)   Acc@5 100.00 ( 99.97)
Epoch: [15][301/313]    Time  0.057 ( 0.042)    Data  0.035 ( 0.025)    Loss 1.8515e-01 (1.7636e-01)    Acc@1  92.97 ( 93.93)   Acc@5 100.00 ( 99.96)
train Loss: 0.1776 Acc: 0.9390
Epoch: [15][  1/313]    Time  0.061 ( 0.043)    Data  0.050 ( 0.026)    Loss 9.4843e-01 (1.8004e-01)    Acc@1  78.12 ( 93.84)   Acc@5  97.66 ( 99.95)
val Loss: 0.9573 Acc: 0.7479

Epoch 16/39
----------
Epoch: [16][  1/313]    Time  0.130 ( 0.130)    Data  0.076 ( 0.076)    Loss 6.6291e-02 (6.6291e-02)    Acc@1  99.22 ( 99.22)   Acc@5 100.00 (100.00)
Epoch: [16][101/313]    Time  0.051 ( 0.049)    Data  0.031 ( 0.030)    Loss 8.0783e-02 (1.2383e-01)    Acc@1  96.09 ( 95.61)   Acc@5 100.00 ( 99.99)
Epoch: [16][201/313]    Time  0.043 ( 0.047)    Data  0.025 ( 0.028)    Loss 1.4447e-01 (1.5196e-01)    Acc@1  94.53 ( 94.69)   Acc@5 100.00 ( 99.97)
Epoch: [16][301/313]    Time  0.045 ( 0.045)    Data  0.024 ( 0.027)    Loss 1.8234e-01 (1.5719e-01)    Acc@1  92.97 ( 94.48)   Acc@5 100.00 ( 99.97)
train Loss: 0.1568 Acc: 0.9448
Epoch: [16][  1/313]    Time  0.042 ( 0.045)    Data  0.035 ( 0.027)    Loss 1.3212e+00 (1.6055e-01)    Acc@1  68.75 ( 94.40)   Acc@5  96.88 ( 99.96)
val Loss: 1.0394 Acc: 0.7469

Epoch 17/39
----------
Epoch: [17][  1/313]    Time  0.096 ( 0.096)    Data  0.071 ( 0.071)    Loss 1.1843e-01 (1.1843e-01)    Acc@1  95.31 ( 95.31)   Acc@5 100.00 (100.00)
Epoch: [17][101/313]    Time  0.041 ( 0.045)    Data  0.024 ( 0.025)    Loss 1.7224e-01 (1.2526e-01)    Acc@1  96.09 ( 95.65)   Acc@5 100.00 ( 99.99)
Epoch: [17][201/313]    Time  0.047 ( 0.045)    Data  0.027 ( 0.027)    Loss 2.4211e-01 (1.4180e-01)    Acc@1  92.19 ( 95.18)   Acc@5 100.00 ( 99.98)
Epoch: [17][301/313]    Time  0.038 ( 0.046)    Data  0.024 ( 0.027)    Loss 2.2922e-01 (1.4516e-01)    Acc@1  89.06 ( 94.98)   Acc@5 100.00 ( 99.98)
train Loss: 0.1475 Acc: 0.9488
Epoch: [17][  1/313]    Time  0.043 ( 0.045)    Data  0.036 ( 0.027)    Loss 9.8574e-01 (1.5017e-01)    Acc@1  78.91 ( 94.83)   Acc@5  95.31 ( 99.97)
val Loss: 1.0589 Acc: 0.7457

Epoch 18/39
----------
Epoch: [18][  1/313]    Time  0.095 ( 0.095)    Data  0.078 ( 0.078)    Loss 1.0244e-01 (1.0244e-01)    Acc@1  96.88 ( 96.88)   Acc@5 100.00 (100.00)
Epoch: [18][101/313]    Time  0.057 ( 0.048)    Data  0.036 ( 0.029)    Loss 1.1561e-01 (1.2190e-01)    Acc@1  96.09 ( 95.84)   Acc@5 100.00 (100.00)
Epoch: [18][201/313]    Time  0.042 ( 0.048)    Data  0.024 ( 0.028)    Loss 1.1848e-01 (1.2786e-01)    Acc@1  96.88 ( 95.57)   Acc@5 100.00 ( 99.99)
Epoch: [18][301/313]    Time  0.036 ( 0.045)    Data  0.024 ( 0.027)    Loss 1.4708e-01 (1.3267e-01)    Acc@1  92.97 ( 95.46)   Acc@5 100.00 ( 99.98)
train Loss: 0.1342 Acc: 0.9541
Epoch: [18][  1/313]    Time  0.044 ( 0.045)    Data  0.039 ( 0.027)    Loss 1.1211e+00 (1.3732e-01)    Acc@1  74.22 ( 95.34)   Acc@5  96.09 ( 99.97)
val Loss: 1.0460 Acc: 0.7482

Epoch 19/39
----------
Epoch: [19][  1/313]    Time  0.097 ( 0.097)    Data  0.074 ( 0.074)    Loss 8.1494e-02 (8.1494e-02)    Acc@1  95.31 ( 95.31)   Acc@5 100.00 (100.00)
Epoch: [19][101/313]    Time  0.047 ( 0.051)    Data  0.025 ( 0.030)    Loss 1.0715e-01 (1.0499e-01)    Acc@1  96.88 ( 96.36)   Acc@5 100.00 ( 99.99)
Epoch: [19][201/313]    Time  0.043 ( 0.047)    Data  0.025 ( 0.027)    Loss 1.1559e-01 (1.1392e-01)    Acc@1  96.09 ( 96.00)   Acc@5 100.00 ( 99.97)
Epoch: [19][301/313]    Time  0.043 ( 0.046)    Data  0.024 ( 0.027)    Loss 2.3479e-01 (1.2129e-01)    Acc@1  92.97 ( 95.73)   Acc@5 100.00 ( 99.98)
train Loss: 0.1221 Acc: 0.9572
Epoch: [19][  1/313]    Time  0.042 ( 0.046)    Data  0.036 ( 0.027)    Loss 9.2973e-01 (1.2470e-01)    Acc@1  76.56 ( 95.66)   Acc@5  98.44 ( 99.97)
val Loss: 1.0249 Acc: 0.7525

Epoch 20/39
----------
Epoch: [20][  1/313]    Time  0.095 ( 0.095)    Data  0.071 ( 0.071)    Loss 7.4647e-02 (7.4647e-02)    Acc@1  97.66 ( 97.66)   Acc@5 100.00 (100.00)
Epoch: [20][101/313]    Time  0.041 ( 0.043)    Data  0.024 ( 0.025)    Loss 8.2358e-02 (8.6790e-02)    Acc@1  95.31 ( 96.88)   Acc@5 100.00 (100.00)
Epoch: [20][201/313]    Time  0.043 ( 0.043)    Data  0.025 ( 0.025)    Loss 1.7668e-01 (1.0335e-01)    Acc@1  95.31 ( 96.33)   Acc@5 100.00 (100.00)
Epoch: [20][301/313]    Time  0.046 ( 0.042)    Data  0.026 ( 0.024)    Loss 1.0795e-01 (1.1677e-01)    Acc@1  95.31 ( 95.84)   Acc@5  99.22 ( 99.99)
train Loss: 0.1178 Acc: 0.9582
Epoch: [20][  1/313]    Time  0.043 ( 0.042)    Data  0.036 ( 0.024)    Loss 9.6861e-01 (1.2050e-01)    Acc@1  74.22 ( 95.75)   Acc@5 100.00 ( 99.99)
val Loss: 1.0801 Acc: 0.7430

Epoch 21/39
----------
Epoch: [21][  1/313]    Time  0.095 ( 0.095)    Data  0.070 ( 0.070)    Loss 4.3595e-02 (4.3595e-02)    Acc@1  99.22 ( 99.22)   Acc@5 100.00 (100.00)
Epoch: [21][101/313]    Time  0.040 ( 0.045)    Data  0.023 ( 0.026)    Loss 5.7769e-02 (8.8141e-02)    Acc@1  98.44 ( 96.95)   Acc@5 100.00 ( 99.99)
Epoch: [21][201/313]    Time  0.055 ( 0.045)    Data  0.036 ( 0.026)    Loss 1.3992e-01 (1.0570e-01)    Acc@1  95.31 ( 96.32)   Acc@5 100.00 ( 99.99)
Epoch: [21][301/313]    Time  0.044 ( 0.050)    Data  0.025 ( 0.030)    Loss 1.0557e-01 (1.1479e-01)    Acc@1  96.09 ( 95.97)   Acc@5 100.00 ( 99.99)
train Loss: 0.1157 Acc: 0.9595
Epoch: [21][  1/313]    Time  0.046 ( 0.050)    Data  0.038 ( 0.029)    Loss 1.1332e+00 (1.1893e-01)    Acc@1  72.66 ( 95.88)   Acc@5  99.22 ( 99.99)
val Loss: 1.0643 Acc: 0.7534

Epoch 22/39
----------
Epoch: [22][  1/313]    Time  0.103 ( 0.103)    Data  0.078 ( 0.078)    Loss 4.7025e-02 (4.7025e-02)    Acc@1  98.44 ( 98.44)   Acc@5 100.00 (100.00)
Epoch: [22][101/313]    Time  0.041 ( 0.043)    Data  0.023 ( 0.025)    Loss 3.7422e-02 (7.9052e-02)    Acc@1  99.22 ( 97.41)   Acc@5 100.00 (100.00)
Epoch: [22][201/313]    Time  0.042 ( 0.043)    Data  0.025 ( 0.025)    Loss 1.2542e-01 (9.3034e-02)    Acc@1  96.09 ( 96.82)   Acc@5 100.00 ( 99.99)
Epoch: [22][301/313]    Time  0.040 ( 0.043)    Data  0.025 ( 0.025)    Loss 2.2373e-01 (1.0455e-01)    Acc@1  92.97 ( 96.43)   Acc@5 100.00 ( 99.98)
train Loss: 0.1046 Acc: 0.9642
Epoch: [22][  1/313]    Time  0.043 ( 0.043)    Data  0.037 ( 0.025)    Loss 1.0088e+00 (1.0745e-01)    Acc@1  75.00 ( 96.35)   Acc@5  99.22 ( 99.98)
val Loss: 1.0466 Acc: 0.7535

Epoch 23/39
----------
Epoch: [23][  1/313]    Time  0.093 ( 0.093)    Data  0.073 ( 0.073)    Loss 1.4730e-02 (1.4730e-02)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [23][101/313]    Time  0.043 ( 0.043)    Data  0.024 ( 0.025)    Loss 9.7806e-02 (7.9875e-02)    Acc@1  95.31 ( 97.23)   Acc@5 100.00 (100.00)
Epoch: [23][201/313]    Time  0.041 ( 0.043)    Data  0.023 ( 0.024)    Loss 6.7124e-02 (8.5176e-02)    Acc@1  99.22 ( 97.08)   Acc@5 100.00 (100.00)
Epoch: [23][301/313]    Time  0.043 ( 0.043)    Data  0.024 ( 0.025)    Loss 1.4677e-01 (9.3313e-02)    Acc@1  96.09 ( 96.78)   Acc@5 100.00 ( 99.99)
train Loss: 0.0950 Acc: 0.9671
Epoch: [23][  1/313]    Time  0.044 ( 0.043)    Data  0.036 ( 0.025)    Loss 1.3954e+00 (9.9108e-02)    Acc@1  69.53 ( 96.62)   Acc@5  96.88 ( 99.98)
val Loss: 1.1568 Acc: 0.7483

Epoch 24/39
----------
Epoch: [24][  1/313]    Time  0.103 ( 0.103)    Data  0.077 ( 0.077)    Loss 1.7456e-01 (1.7456e-01)    Acc@1  92.97 ( 92.97)   Acc@5 100.00 (100.00)
Epoch: [24][101/313]    Time  0.044 ( 0.043)    Data  0.024 ( 0.025)    Loss 4.5307e-02 (8.6322e-02)    Acc@1  99.22 ( 96.88)   Acc@5 100.00 (100.00)
Epoch: [24][201/313]    Time  0.041 ( 0.042)    Data  0.025 ( 0.024)    Loss 2.3725e-02 (8.2818e-02)    Acc@1  99.22 ( 97.12)   Acc@5 100.00 ( 99.99)
Epoch: [24][301/313]    Time  0.040 ( 0.042)    Data  0.023 ( 0.024)    Loss 1.2612e-01 (9.9255e-02)    Acc@1  96.88 ( 96.55)   Acc@5 100.00 ( 99.98)
train Loss: 0.0999 Acc: 0.9651
Epoch: [24][  1/313]    Time  0.044 ( 0.042)    Data  0.035 ( 0.024)    Loss 1.1991e+00 (1.0342e-01)    Acc@1  76.56 ( 96.44)   Acc@5  95.31 ( 99.97)
val Loss: 1.0434 Acc: 0.7555

Epoch 25/39
----------
Epoch: [25][  1/313]    Time  0.096 ( 0.096)    Data  0.072 ( 0.072)    Loss 1.3640e-01 (1.3640e-01)    Acc@1  94.53 ( 94.53)   Acc@5 100.00 (100.00)
Epoch: [25][101/313]    Time  0.043 ( 0.043)    Data  0.025 ( 0.025)    Loss 7.3611e-02 (7.6571e-02)    Acc@1  96.09 ( 97.26)   Acc@5 100.00 (100.00)
Epoch: [25][201/313]    Time  0.040 ( 0.043)    Data  0.024 ( 0.024)    Loss 1.1330e-01 (8.2443e-02)    Acc@1  96.09 ( 97.10)   Acc@5 100.00 (100.00)
Epoch: [25][301/313]    Time  0.036 ( 0.042)    Data  0.024 ( 0.025)    Loss 6.8394e-02 (8.9053e-02)    Acc@1  96.09 ( 96.87)   Acc@5 100.00 (100.00)
train Loss: 0.0895 Acc: 0.9686
Epoch: [25][  1/313]    Time  0.041 ( 0.042)    Data  0.036 ( 0.025)    Loss 1.0690e+00 (9.2659e-02)    Acc@1  76.56 ( 96.80)   Acc@5  98.44 (100.00)
val Loss: 1.0689 Acc: 0.7600

Epoch 26/39
----------
Epoch: [26][  1/313]    Time  0.093 ( 0.093)    Data  0.073 ( 0.073)    Loss 3.4858e-02 (3.4858e-02)    Acc@1  97.66 ( 97.66)   Acc@5 100.00 (100.00)
Epoch: [26][101/313]    Time  0.041 ( 0.043)    Data  0.024 ( 0.025)    Loss 3.9132e-02 (6.1744e-02)    Acc@1  98.44 ( 97.83)   Acc@5 100.00 ( 99.99)
Epoch: [26][201/313]    Time  0.042 ( 0.043)    Data  0.024 ( 0.024)    Loss 1.1342e-01 (6.7992e-02)    Acc@1  96.09 ( 97.65)   Acc@5 100.00 (100.00)
Epoch: [26][301/313]    Time  0.039 ( 0.042)    Data  0.022 ( 0.024)    Loss 1.2893e-01 (8.1380e-02)    Acc@1  95.31 ( 97.13)   Acc@5 100.00 (100.00)
train Loss: 0.0838 Acc: 0.9706
Epoch: [26][  1/313]    Time  0.044 ( 0.042)    Data  0.035 ( 0.024)    Loss 1.0815e+00 (8.7025e-02)    Acc@1  75.78 ( 96.99)   Acc@5  96.88 ( 99.98)
val Loss: 1.1031 Acc: 0.7474

Epoch 27/39
----------
Epoch: [27][  1/313]    Time  0.107 ( 0.107)    Data  0.079 ( 0.079)    Loss 6.9684e-02 (6.9684e-02)    Acc@1  96.88 ( 96.88)   Acc@5 100.00 (100.00)
Epoch: [27][101/313]    Time  0.039 ( 0.043)    Data  0.023 ( 0.025)    Loss 6.2519e-02 (7.9814e-02)    Acc@1  96.88 ( 97.15)   Acc@5 100.00 ( 99.99)
Epoch: [27][201/313]    Time  0.040 ( 0.042)    Data  0.023 ( 0.024)    Loss 5.2445e-02 (8.4075e-02)    Acc@1  97.66 ( 97.05)   Acc@5 100.00 ( 99.99)
Epoch: [27][301/313]    Time  0.041 ( 0.042)    Data  0.025 ( 0.025)    Loss 2.2170e-01 (9.2222e-02)    Acc@1  91.41 ( 96.80)   Acc@5 100.00 ( 99.99)
train Loss: 0.0932 Acc: 0.9677
Epoch: [27][  1/313]    Time  0.043 ( 0.042)    Data  0.037 ( 0.025)    Loss 1.1132e+00 (9.6449e-02)    Acc@1  74.22 ( 96.70)   Acc@5  97.66 ( 99.98)
val Loss: 1.1134 Acc: 0.7510

Epoch 28/39
----------
Epoch: [28][  1/313]    Time  0.085 ( 0.085)    Data  0.072 ( 0.072)    Loss 1.1257e-01 (1.1257e-01)    Acc@1  96.09 ( 96.09)   Acc@5 100.00 (100.00)
Epoch: [28][101/313]    Time  0.039 ( 0.039)    Data  0.023 ( 0.025)    Loss 4.6588e-02 (7.0381e-02)    Acc@1  96.88 ( 97.63)   Acc@5 100.00 ( 99.99)
Epoch: [28][201/313]    Time  0.040 ( 0.040)    Data  0.024 ( 0.025)    Loss 5.7929e-02 (7.7563e-02)    Acc@1  98.44 ( 97.36)   Acc@5 100.00 ( 99.99)
Epoch: [28][301/313]    Time  0.040 ( 0.041)    Data  0.024 ( 0.024)    Loss 1.8064e-01 (8.6634e-02)    Acc@1  93.75 ( 97.02)   Acc@5 100.00 ( 99.99)
train Loss: 0.0871 Acc: 0.9698
Epoch: [28][  1/313]    Time  0.045 ( 0.041)    Data  0.038 ( 0.025)    Loss 1.0271e+00 (9.0108e-02)    Acc@1  76.56 ( 96.92)   Acc@5  97.66 ( 99.98)
val Loss: 1.2486 Acc: 0.7409

Epoch 29/39
----------
Epoch: [29][  1/313]    Time  0.093 ( 0.093)    Data  0.072 ( 0.072)    Loss 8.8379e-02 (8.8379e-02)    Acc@1  97.66 ( 97.66)   Acc@5 100.00 (100.00)
Epoch: [29][101/313]    Time  0.041 ( 0.044)    Data  0.025 ( 0.026)    Loss 6.2410e-02 (7.3010e-02)    Acc@1  97.66 ( 97.61)   Acc@5 100.00 (100.00)
Epoch: [29][201/313]    Time  0.041 ( 0.044)    Data  0.024 ( 0.026)    Loss 6.0902e-02 (7.6817e-02)    Acc@1  97.66 ( 97.38)   Acc@5 100.00 (100.00)
Epoch: [29][301/313]    Time  0.041 ( 0.044)    Data  0.024 ( 0.025)    Loss 1.0438e-01 (8.5357e-02)    Acc@1  97.66 ( 97.07)   Acc@5 100.00 ( 99.99)
train Loss: 0.0863 Acc: 0.9702
Epoch: [29][  1/313]    Time  0.044 ( 0.044)    Data  0.038 ( 0.025)    Loss 1.0042e+00 (8.9254e-02)    Acc@1  78.91 ( 96.96)   Acc@5  96.88 ( 99.98)
val Loss: 1.1633 Acc: 0.7485

Epoch 30/39
----------
Epoch: [30][  1/313]    Time  0.085 ( 0.085)    Data  0.072 ( 0.072)    Loss 6.1452e-02 (6.1452e-02)    Acc@1  98.44 ( 98.44)   Acc@5 100.00 (100.00)
Epoch: [30][101/313]    Time  0.042 ( 0.041)    Data  0.026 ( 0.025)    Loss 3.8363e-02 (7.6703e-02)    Acc@1  99.22 ( 97.37)   Acc@5 100.00 ( 99.99)
Epoch: [30][201/313]    Time  0.043 ( 0.042)    Data  0.026 ( 0.025)    Loss 2.6287e-02 (7.7240e-02)    Acc@1  99.22 ( 97.44)   Acc@5 100.00 ( 99.99)
Epoch: [30][301/313]    Time  0.041 ( 0.043)    Data  0.024 ( 0.026)    Loss 1.4560e-01 (8.4346e-02)    Acc@1  96.09 ( 97.13)   Acc@5 100.00 ( 99.99)
train Loss: 0.0854 Acc: 0.9709
Epoch: [30][  1/313]    Time  0.044 ( 0.043)    Data  0.036 ( 0.026)    Loss 9.3637e-01 (8.8158e-02)    Acc@1  77.34 ( 97.03)   Acc@5  96.88 ( 99.98)
val Loss: 1.1573 Acc: 0.7502

Epoch 31/39
----------
Epoch: [31][  1/313]    Time  0.109 ( 0.109)    Data  0.077 ( 0.077)    Loss 9.5639e-02 (9.5639e-02)    Acc@1  96.09 ( 96.09)   Acc@5 100.00 (100.00)
Epoch: [31][101/313]    Time  0.042 ( 0.047)    Data  0.025 ( 0.027)    Loss 1.1565e-01 (8.4990e-02)    Acc@1  96.88 ( 97.09)   Acc@5 100.00 ( 99.98)
Epoch: [31][201/313]    Time  0.036 ( 0.044)    Data  0.024 ( 0.026)    Loss 1.2953e-01 (8.4472e-02)    Acc@1  96.88 ( 97.10)   Acc@5 100.00 ( 99.99)
Epoch: [31][301/313]    Time  0.051 ( 0.045)    Data  0.036 ( 0.028)    Loss 3.9980e-02 (8.6906e-02)    Acc@1  99.22 ( 97.06)   Acc@5 100.00 ( 99.99)
train Loss: 0.0872 Acc: 0.9703
Epoch: [31][  1/313]    Time  0.048 ( 0.045)    Data  0.041 ( 0.028)    Loss 9.6983e-01 (9.0015e-02)    Acc@1  75.78 ( 96.96)   Acc@5 100.00 ( 99.99)
val Loss: 1.1417 Acc: 0.7560

Epoch 32/39
----------
Epoch: [32][  1/313]    Time  0.091 ( 0.091)    Data  0.073 ( 0.073)    Loss 1.0627e-01 (1.0627e-01)    Acc@1  97.66 ( 97.66)   Acc@5 100.00 (100.00)
Epoch: [32][101/313]    Time  0.047 ( 0.047)    Data  0.025 ( 0.028)    Loss 4.6776e-02 (6.8041e-02)    Acc@1  99.22 ( 97.69)   Acc@5 100.00 (100.00)
Epoch: [32][201/313]    Time  0.037 ( 0.044)    Data  0.022 ( 0.027)    Loss 8.6430e-02 (7.2156e-02)    Acc@1  97.66 ( 97.52)   Acc@5 100.00 ( 99.99)
Epoch: [32][301/313]    Time  0.045 ( 0.045)    Data  0.028 ( 0.028)    Loss 2.2222e-01 (8.1146e-02)    Acc@1  94.53 ( 97.23)   Acc@5 100.00 ( 99.99)
train Loss: 0.0823 Acc: 0.9719
Epoch: [32][  1/313]    Time  0.047 ( 0.045)    Data  0.039 ( 0.028)    Loss 1.4267e+00 (8.6541e-02)    Acc@1  66.41 ( 97.09)   Acc@5  96.88 ( 99.98)
val Loss: 1.1398 Acc: 0.7477

Epoch 33/39
----------
Epoch: [33][  1/313]    Time  0.098 ( 0.098)    Data  0.076 ( 0.076)    Loss 3.9107e-02 (3.9107e-02)    Acc@1  98.44 ( 98.44)   Acc@5 100.00 (100.00)
Epoch: [33][101/313]    Time  0.042 ( 0.043)    Data  0.025 ( 0.025)    Loss 4.2700e-02 (6.0062e-02)    Acc@1  98.44 ( 98.02)   Acc@5 100.00 ( 99.99)
Epoch: [33][201/313]    Time  0.042 ( 0.043)    Data  0.024 ( 0.024)    Loss 7.3895e-02 (6.2954e-02)    Acc@1  97.66 ( 97.94)   Acc@5 100.00 (100.00)
Epoch: [33][301/313]    Time  0.043 ( 0.044)    Data  0.025 ( 0.025)    Loss 1.2415e-01 (6.7905e-02)    Acc@1  95.31 ( 97.76)   Acc@5 100.00 ( 99.99)
train Loss: 0.0688 Acc: 0.9773
Epoch: [33][  1/313]    Time  0.096 ( 0.045)    Data  0.087 ( 0.026)    Loss 1.4176e+00 (7.3120e-02)    Acc@1  74.22 ( 97.65)   Acc@5  96.09 ( 99.98)
val Loss: 1.1795 Acc: 0.7541

Epoch 34/39
----------
Epoch: [34][  1/313]    Time  0.104 ( 0.104)    Data  0.076 ( 0.076)    Loss 1.1607e-01 (1.1607e-01)    Acc@1  96.88 ( 96.88)   Acc@5 100.00 (100.00)
Epoch: [34][101/313]    Time  0.044 ( 0.044)    Data  0.026 ( 0.025)    Loss 7.1407e-02 (5.9796e-02)    Acc@1  97.66 ( 98.06)   Acc@5 100.00 (100.00)
Epoch: [34][201/313]    Time  0.041 ( 0.043)    Data  0.024 ( 0.025)    Loss 6.4603e-02 (6.5227e-02)    Acc@1  97.66 ( 97.85)   Acc@5 100.00 (100.00)
Epoch: [34][301/313]    Time  0.041 ( 0.043)    Data  0.023 ( 0.025)    Loss 9.8715e-02 (7.3882e-02)    Acc@1  96.88 ( 97.49)   Acc@5 100.00 ( 99.99)
train Loss: 0.0746 Acc: 0.9746
Epoch: [34][  1/313]    Time  0.051 ( 0.043)    Data  0.041 ( 0.025)    Loss 1.5000e+00 (7.9164e-02)    Acc@1  73.44 ( 97.39)   Acc@5  96.88 ( 99.98)
val Loss: 1.1916 Acc: 0.7488

Epoch 35/39
----------
Epoch: [35][  1/313]    Time  0.097 ( 0.097)    Data  0.082 ( 0.082)    Loss 9.2178e-02 (9.2178e-02)    Acc@1  96.88 ( 96.88)   Acc@5 100.00 (100.00)
Epoch: [35][101/313]    Time  0.041 ( 0.039)    Data  0.025 ( 0.025)    Loss 2.7994e-02 (6.9285e-02)    Acc@1  99.22 ( 97.68)   Acc@5 100.00 ( 99.99)
Epoch: [35][201/313]    Time  0.041 ( 0.042)    Data  0.024 ( 0.025)    Loss 5.5424e-02 (6.7421e-02)    Acc@1  96.88 ( 97.72)   Acc@5 100.00 (100.00)
Epoch: [35][301/313]    Time  0.041 ( 0.046)    Data  0.024 ( 0.029)    Loss 1.1797e-01 (7.7370e-02)    Acc@1  96.88 ( 97.39)   Acc@5 100.00 ( 99.99)
train Loss: 0.0779 Acc: 0.9737
Epoch: [35][  1/313]    Time  0.048 ( 0.045)    Data  0.039 ( 0.029)    Loss 1.2485e+00 (8.1681e-02)    Acc@1  69.53 ( 97.28)   Acc@5  97.66 ( 99.99)
val Loss: 1.0789 Acc: 0.7643

Epoch 36/39
----------
Epoch: [36][  1/313]    Time  0.107 ( 0.107)    Data  0.083 ( 0.083)    Loss 8.2159e-02 (8.2159e-02)    Acc@1  96.88 ( 96.88)   Acc@5  99.22 ( 99.22)
Epoch: [36][101/313]    Time  0.035 ( 0.043)    Data  0.023 ( 0.027)    Loss 1.0388e-01 (6.3044e-02)    Acc@1  96.88 ( 97.82)   Acc@5 100.00 ( 99.99)
Epoch: [36][201/313]    Time  0.040 ( 0.042)    Data  0.024 ( 0.026)    Loss 3.5782e-02 (7.0318e-02)    Acc@1  99.22 ( 97.59)   Acc@5 100.00 ( 99.99)
Epoch: [36][301/313]    Time  0.042 ( 0.042)    Data  0.024 ( 0.025)    Loss 1.6510e-01 (7.3938e-02)    Acc@1  95.31 ( 97.48)   Acc@5 100.00 ( 99.99)
train Loss: 0.0748 Acc: 0.9746
Epoch: [36][  1/313]    Time  0.044 ( 0.042)    Data  0.037 ( 0.025)    Loss 1.0406e+00 (7.7864e-02)    Acc@1  75.00 ( 97.38)   Acc@5  97.66 ( 99.99)
val Loss: 1.1641 Acc: 0.7489

Epoch 37/39
----------
Epoch: [37][  1/313]    Time  0.098 ( 0.098)    Data  0.075 ( 0.075)    Loss 1.1113e-01 (1.1113e-01)    Acc@1  96.09 ( 96.09)   Acc@5 100.00 (100.00)
Epoch: [37][101/313]    Time  0.041 ( 0.046)    Data  0.023 ( 0.026)    Loss 9.2767e-02 (6.6497e-02)    Acc@1  97.66 ( 97.90)   Acc@5 100.00 (100.00)
Epoch: [37][201/313]    Time  0.052 ( 0.047)    Data  0.036 ( 0.028)    Loss 6.3624e-02 (6.8509e-02)    Acc@1  98.44 ( 97.80)   Acc@5 100.00 (100.00)
Epoch: [37][301/313]    Time  0.042 ( 0.046)    Data  0.024 ( 0.027)    Loss 7.3864e-02 (7.5748e-02)    Acc@1  97.66 ( 97.50)   Acc@5 100.00 (100.00)
train Loss: 0.0759 Acc: 0.9749
Epoch: [37][  1/313]    Time  0.044 ( 0.046)    Data  0.037 ( 0.027)    Loss 1.1040e+00 (7.9224e-02)    Acc@1  73.44 ( 97.41)   Acc@5  96.88 ( 99.99)
val Loss: 1.1778 Acc: 0.7575

Epoch 38/39
----------
Epoch: [38][  1/313]    Time  0.505 ( 0.505)    Data  0.489 ( 0.489)    Loss 3.4767e-02 (3.4767e-02)    Acc@1  98.44 ( 98.44)   Acc@5 100.00 (100.00)
Epoch: [38][101/313]    Time  0.042 ( 0.052)    Data  0.025 ( 0.033)    Loss 4.6133e-02 (6.7899e-02)    Acc@1  97.66 ( 97.72)   Acc@5 100.00 (100.00)
Epoch: [38][201/313]    Time  0.037 ( 0.047)    Data  0.024 ( 0.029)    Loss 9.7034e-02 (6.7478e-02)    Acc@1  97.66 ( 97.74)   Acc@5 100.00 (100.00)
Epoch: [38][301/313]    Time  0.040 ( 0.044)    Data  0.024 ( 0.027)    Loss 3.5974e-02 (7.6868e-02)    Acc@1 100.00 ( 97.37)   Acc@5 100.00 ( 99.99)
train Loss: 0.0779 Acc: 0.9733
Epoch: [38][  1/313]    Time  0.043 ( 0.044)    Data  0.037 ( 0.027)    Loss 6.7779e-01 (7.9839e-02)    Acc@1  79.69 ( 97.27)   Acc@5  99.22 ( 99.99)
val Loss: 1.0518 Acc: 0.7613

Epoch 39/39
----------
Epoch: [39][  1/313]    Time  0.092 ( 0.092)    Data  0.072 ( 0.072)    Loss 3.4582e-02 (3.4582e-02)    Acc@1  99.22 ( 99.22)   Acc@5 100.00 (100.00)
Epoch: [39][101/313]    Time  0.040 ( 0.041)    Data  0.026 ( 0.025)    Loss 1.4679e-01 (5.2834e-02)    Acc@1  96.09 ( 98.24)   Acc@5 100.00 (100.00)
Epoch: [39][201/313]    Time  0.043 ( 0.045)    Data  0.025 ( 0.028)    Loss 9.6080e-02 (5.7148e-02)    Acc@1  96.09 ( 98.10)   Acc@5 100.00 (100.00)
Epoch: [39][301/313]    Time  0.044 ( 0.044)    Data  0.025 ( 0.027)    Loss 1.5044e-01 (6.4102e-02)    Acc@1  96.09 ( 97.86)   Acc@5 100.00 ( 99.99)
train Loss: 0.0650 Acc: 0.9783
Epoch: [39][  1/313]    Time  0.044 ( 0.044)    Data  0.037 ( 0.027)    Loss 1.5849e+00 (6.9831e-02)    Acc@1  67.97 ( 97.74)   Acc@5  96.09 ( 99.98)
val Loss: 1.2184 Acc: 0.7462

Training complete in 10m 57s
Best val Acc: 0.764300
Test Loss: 0.215777

Test Accuracy of airplane: 79% (803/1015)
Test Accuracy of automobile: 85% (806/939)
Test Accuracy of  bird: 74% (705/949)
Test Accuracy of   cat: 59% (630/1056)
Test Accuracy of  deer: 65% (664/1007)
Test Accuracy of   dog: 67% (707/1045)
Test Accuracy of  frog: 84% (858/1020)
Test Accuracy of horse: 76% (734/961)
Test Accuracy of  ship: 85% (848/997)
Test Accuracy of truck: 87% (888/1011)

Test Accuracy (Overall): 76% (7643/10000)