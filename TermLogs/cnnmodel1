PS C:\mygit\MultiModalClassifier> python .\TorchClassifier\myTorchTrainer.py --data_name 'CIFAR10' --data_type 'torchvisiondataset' --data_path "C:\mygit\TrainingData\data\" --model_name 'cnnmodel1' --learningratename 'ConstantLR' --optimizer 'SGD'                            
2.2.1
Torch Version:  2.2.1
Torchvision Version:  0.17.1
Output path: ./outputs/CIFAR10_cnnmodel1_0910
Num GPUs: 1
0
NVIDIA GeForce RTX 3070 Ti
True
Files already downloaded and verified
Files already downloaded and verified
Number of training examples: 50000
Number of testing examples: 10000
CNNNet1(
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (fc1): Linear(in_features=1024, out_features=500, bias=True)
  (fc2): Linear(in_features=500, out_features=10, bias=True)
  (dropout): Dropout(p=0.25, inplace=False)
)
========================================================================================================================
Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable
========================================================================================================================
CNNNet1 (CNNNet1)                        [128, 3, 224, 224]   [6272, 10]           --                   True
├─Conv2d (conv1)                         [128, 3, 224, 224]   [128, 16, 224, 224]  448                  True
├─MaxPool2d (pool)                       [128, 16, 224, 224]  [128, 16, 112, 112]  --                   --
├─Conv2d (conv2)                         [128, 16, 112, 112]  [128, 32, 112, 112]  4,640                True
├─MaxPool2d (pool)                       [128, 32, 112, 112]  [128, 32, 56, 56]    --                   --
├─Conv2d (conv3)                         [128, 32, 56, 56]    [128, 64, 56, 56]    18,496               True
├─MaxPool2d (pool)                       [128, 64, 56, 56]    [128, 64, 28, 28]    --                   --
├─Dropout (dropout)                      [6272, 1024]         [6272, 1024]         --                   --
├─Linear (fc1)                           [6272, 1024]         [6272, 500]          512,500              True
├─Dropout (dropout)                      [6272, 500]          [6272, 500]          --                   --
├─Linear (fc2)                           [6272, 500]          [6272, 10]           5,010                True
========================================================================================================================
Total params: 541,094
Trainable params: 541,094
Non-trainable params: 0
Total mult-adds (Units.GIGABYTES): 21.00
========================================================================================================================
Input size (MB): 77.07
Forward/backward pass size (MB): 1464.24
Params size (MB): 2.16
Estimated Total Size (MB): 1543.47
========================================================================================================================
=> no checkpoint found at 'outputs/imagenet_blurred_resnet50_0328/model_best.pth.tar'
Epoch 0/39
----------
Epoch: [0][  1/313]     Time  0.549 ( 0.549)    Data  0.094 ( 0.094)    Loss 2.3055e+00 (2.3055e+00)    Acc@1  11.72 ( 11.72)   Acc@5  46.88 ( 46.88)
STAGE:2024-03-26 15:55:57 144128:43652 C:\cb\pytorch_1000000000000\work\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:314] Completed Stage: Warm Up
STAGE:2024-03-26 15:55:58 144128:43652 C:\cb\pytorch_1000000000000\work\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:320] Completed Stage: Collection
STAGE:2024-03-26 15:55:58 144128:43652 C:\cb\pytorch_1000000000000\work\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:324] Completed Stage: Post Processing
STAGE:2024-03-26 15:55:59 144128:43652 C:\cb\pytorch_1000000000000\work\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:314] Completed Stage: Warm Up
STAGE:2024-03-26 15:56:00 144128:43652 C:\cb\pytorch_1000000000000\work\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:320] Completed Stage: Collection
STAGE:2024-03-26 15:56:00 144128:43652 C:\cb\pytorch_1000000000000\work\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:324] Completed Stage: Post Processing
Epoch: [0][101/313]     Time  0.029 ( 0.074)    Data  0.024 ( 0.065)    Loss 1.8710e+00 (2.1113e+00)    Acc@1  30.47 ( 21.52)   Acc@5  80.47 ( 70.33)
Epoch: [0][201/313]     Time  0.034 ( 0.054)    Data  0.023 ( 0.044)    Loss 1.4604e+00 (1.8960e+00)    Acc@1  42.19 ( 30.03)   Acc@5  91.41 ( 79.19)
Epoch: [0][301/313]     Time  0.037 ( 0.048)    Data  0.022 ( 0.037)    Loss 1.5259e+00 (1.7592e+00)    Acc@1  43.75 ( 35.34)   Acc@5  89.06 ( 83.36)
train Loss: 1.7482 Acc: 0.3576
Epoch: [0][  1/313]     Time  0.045 ( 0.048)    Data  0.035 ( 0.036)    Loss 1.4683e+00 (1.7473e+00)    Acc@1  42.19 ( 35.78)   Acc@5  90.62 ( 83.68)
val Loss: 1.3752 Acc: 0.5004

Epoch 1/39
----------
Epoch: [1][  1/313]     Time  0.100 ( 0.100)    Data  0.075 ( 0.075)    Loss 1.4376e+00 (1.4376e+00)    Acc@1  51.56 ( 51.56)   Acc@5  92.97 ( 92.97)
Epoch: [1][101/313]     Time  0.035 ( 0.037)    Data  0.024 ( 0.024)    Loss 1.1530e+00 (1.3289e+00)    Acc@1  55.47 ( 52.01)   Acc@5  96.88 ( 94.13)
Epoch: [1][201/313]     Time  0.036 ( 0.037)    Data  0.024 ( 0.024)    Loss 1.2511e+00 (1.3086e+00)    Acc@1  58.59 ( 52.99)   Acc@5  93.75 ( 94.11)
Epoch: [1][301/313]     Time  0.034 ( 0.037)    Data  0.023 ( 0.024)    Loss 1.2273e+00 (1.2760e+00)    Acc@1  57.03 ( 54.26)   Acc@5  96.09 ( 94.44)
train Loss: 1.2709 Acc: 0.5445
Epoch: [1][  1/313]     Time  0.041 ( 0.037)    Data  0.036 ( 0.024)    Loss 1.1126e+00 (1.2704e+00)    Acc@1  58.59 ( 54.47)   Acc@5  96.09 ( 94.47)
val Loss: 1.0888 Acc: 0.6140

Epoch 2/39
----------
Epoch: [2][  1/313]     Time  0.096 ( 0.096)    Data  0.072 ( 0.072)    Loss 1.1696e+00 (1.1696e+00)    Acc@1  55.47 ( 55.47)   Acc@5  96.09 ( 96.09)
Epoch: [2][101/313]     Time  0.034 ( 0.037)    Data  0.023 ( 0.024)    Loss 9.6019e-01 (1.1082e+00)    Acc@1  71.88 ( 60.55)   Acc@5  93.75 ( 96.26)
Epoch: [2][201/313]     Time  0.036 ( 0.036)    Data  0.023 ( 0.024)    Loss 9.0829e-01 (1.0911e+00)    Acc@1  70.31 ( 61.02)   Acc@5  99.22 ( 96.43)
Epoch: [2][301/313]     Time  0.046 ( 0.036)    Data  0.023 ( 0.024)    Loss 9.8202e-01 (1.0809e+00)    Acc@1  67.97 ( 61.35)   Acc@5  96.88 ( 96.40)
train Loss: 1.0790 Acc: 0.6141
Epoch: [2][  1/313]     Time  0.076 ( 0.037)    Data  0.041 ( 0.024)    Loss 1.0929e+00 (1.0790e+00)    Acc@1  62.50 ( 61.41)   Acc@5  95.31 ( 96.41)
val Loss: 0.9946 Acc: 0.6502

Epoch 3/39
----------
Epoch: [3][  1/313]     Time  0.095 ( 0.095)    Data  0.071 ( 0.071)    Loss 8.8284e-01 (8.8284e-01)    Acc@1  67.97 ( 67.97)   Acc@5  98.44 ( 98.44)
Epoch: [3][101/313]     Time  0.034 ( 0.037)    Data  0.024 ( 0.024)    Loss 9.5999e-01 (9.9716e-01)    Acc@1  63.28 ( 64.84)   Acc@5  98.44 ( 97.04)
Epoch: [3][201/313]     Time  0.034 ( 0.036)    Data  0.023 ( 0.023)    Loss 1.0800e+00 (9.8182e-01)    Acc@1  60.16 ( 65.44)   Acc@5  95.31 ( 97.10)
Epoch: [3][301/313]     Time  0.048 ( 0.036)    Data  0.023 ( 0.023)    Loss 1.0581e+00 (9.7833e-01)    Acc@1  64.06 ( 65.53)   Acc@5  92.19 ( 97.06)
train Loss: 0.9797 Acc: 0.6548
Epoch: [3][  1/313]     Time  0.038 ( 0.036)    Data  0.033 ( 0.023)    Loss 8.8149e-01 (9.7938e-01)    Acc@1  71.88 ( 65.50)   Acc@5  96.88 ( 97.04)
val Loss: 0.9156 Acc: 0.6839

Epoch 4/39
----------
Epoch: [4][  1/313]     Time  0.094 ( 0.094)    Data  0.070 ( 0.070)    Loss 7.9555e-01 (7.9555e-01)    Acc@1  69.53 ( 69.53)   Acc@5  99.22 ( 99.22)
Epoch: [4][101/313]     Time  0.033 ( 0.036)    Data  0.022 ( 0.024)    Loss 1.2521e+00 (1.1312e+00)    Acc@1  56.25 ( 60.40)   Acc@5  96.09 ( 95.97)
Epoch: [4][201/313]     Time  0.036 ( 0.036)    Data  0.023 ( 0.023)    Loss 1.2021e+00 (1.1640e+00)    Acc@1  55.47 ( 59.31)   Acc@5  94.53 ( 95.64)
Epoch: [4][301/313]     Time  0.034 ( 0.036)    Data  0.023 ( 0.023)    Loss 1.1595e+00 (1.1680e+00)    Acc@1  63.28 ( 59.24)   Acc@5  96.09 ( 95.50)
train Loss: 1.1686 Acc: 0.5923
Epoch: [4][  1/313]     Time  0.042 ( 0.036)    Data  0.034 ( 0.023)    Loss 9.6055e-01 (1.1679e+00)    Acc@1  63.28 ( 59.24)   Acc@5  97.66 ( 95.49)
val Loss: 1.1037 Acc: 0.6279

Epoch 5/39
----------
Epoch: [5][  1/313]     Time  0.100 ( 0.100)    Data  0.072 ( 0.072)    Loss 1.0037e+00 (1.0037e+00)    Acc@1  63.28 ( 63.28)   Acc@5  96.09 ( 96.09)
Epoch: [5][101/313]     Time  0.034 ( 0.036)    Data  0.024 ( 0.024)    Loss 1.1838e+00 (1.1251e+00)    Acc@1  57.03 ( 60.49)   Acc@5  93.75 ( 95.76)
Epoch: [5][201/313]     Time  0.038 ( 0.035)    Data  0.025 ( 0.023)    Loss 1.0410e+00 (1.1413e+00)    Acc@1  59.38 ( 60.13)   Acc@5  96.88 ( 95.65)
Epoch: [5][301/313]     Time  0.033 ( 0.036)    Data  0.023 ( 0.023)    Loss 1.2243e+00 (1.1601e+00)    Acc@1  61.72 ( 59.70)   Acc@5  94.53 ( 95.47)
train Loss: 1.1617 Acc: 0.5968
Epoch: [5][  1/313]     Time  0.041 ( 0.036)    Data  0.035 ( 0.023)    Loss 1.1622e+00 (1.1617e+00)    Acc@1  62.50 ( 59.69)   Acc@5  94.53 ( 95.43)
val Loss: 1.0771 Acc: 0.6304

Epoch 6/39
----------
Epoch: [6][  1/313]     Time  0.094 ( 0.094)    Data  0.071 ( 0.071)    Loss 9.9158e-01 (9.9158e-01)    Acc@1  64.06 ( 64.06)   Acc@5  95.31 ( 95.31)
Epoch: [6][101/313]     Time  0.034 ( 0.036)    Data  0.023 ( 0.024)    Loss 1.0612e+00 (1.1383e+00)    Acc@1  60.16 ( 60.92)   Acc@5  96.88 ( 95.71)
Epoch: [6][201/313]     Time  0.035 ( 0.035)    Data  0.022 ( 0.023)    Loss 1.0176e+00 (1.1409e+00)    Acc@1  66.41 ( 60.67)   Acc@5  95.31 ( 95.62)
Epoch: [6][301/313]     Time  0.033 ( 0.036)    Data  0.023 ( 0.024)    Loss 1.1862e+00 (1.1463e+00)    Acc@1  59.38 ( 60.44)   Acc@5  95.31 ( 95.53)
train Loss: 1.1445 Acc: 0.6055
Epoch: [6][  1/313]     Time  0.041 ( 0.036)    Data  0.036 ( 0.024)    Loss 1.1031e+00 (1.1444e+00)    Acc@1  60.16 ( 60.55)   Acc@5  94.53 ( 95.51)
val Loss: 1.0659 Acc: 0.6281

Epoch 7/39
----------
Epoch: [7][  1/313]     Time  0.094 ( 0.094)    Data  0.071 ( 0.071)    Loss 1.0889e+00 (1.0889e+00)    Acc@1  60.94 ( 60.94)   Acc@5  94.53 ( 94.53)
Epoch: [7][101/313]     Time  0.035 ( 0.037)    Data  0.024 ( 0.024)    Loss 1.1389e+00 (1.1074e+00)    Acc@1  60.16 ( 61.55)   Acc@5  96.88 ( 95.73)
Epoch: [7][201/313]     Time  0.036 ( 0.036)    Data  0.023 ( 0.024)    Loss 9.5651e-01 (1.1399e+00)    Acc@1  68.75 ( 60.59)   Acc@5  98.44 ( 95.42)
Epoch: [7][301/313]     Time  0.033 ( 0.037)    Data  0.023 ( 0.024)    Loss 1.0715e+00 (1.1531e+00)    Acc@1  62.50 ( 60.15)   Acc@5  96.88 ( 95.23)
train Loss: 1.1581 Acc: 0.6001
Epoch: [7][  1/313]     Time  0.043 ( 0.037)    Data  0.034 ( 0.024)    Loss 1.2053e+00 (1.1583e+00)    Acc@1  54.69 ( 59.99)   Acc@5  94.53 ( 95.20)
val Loss: 1.2385 Acc: 0.5793

Epoch 8/39
----------
Epoch: [8][  1/313]     Time  0.094 ( 0.094)    Data  0.070 ( 0.070)    Loss 1.2822e+00 (1.2822e+00)    Acc@1  57.81 ( 57.81)   Acc@5  95.31 ( 95.31)
Epoch: [8][101/313]     Time  0.034 ( 0.036)    Data  0.022 ( 0.024)    Loss 1.1869e+00 (1.1764e+00)    Acc@1  53.12 ( 60.55)   Acc@5  96.88 ( 95.11)
Epoch: [8][201/313]     Time  0.036 ( 0.036)    Data  0.023 ( 0.023)    Loss 1.2933e+00 (1.1692e+00)    Acc@1  52.34 ( 60.23)   Acc@5  96.09 ( 95.13)
Epoch: [8][301/313]     Time  0.038 ( 0.036)    Data  0.024 ( 0.023)    Loss 1.1100e+00 (1.1613e+00)    Acc@1  58.59 ( 60.42)   Acc@5  96.09 ( 95.18)
train Loss: 1.1618 Acc: 0.6039
Epoch: [8][  1/313]     Time  0.045 ( 0.036)    Data  0.039 ( 0.024)    Loss 1.1582e+00 (1.1618e+00)    Acc@1  60.94 ( 60.39)   Acc@5  96.88 ( 95.19)
val Loss: 1.1463 Acc: 0.5960

Epoch 9/39
----------
Epoch: [9][  1/313]     Time  0.097 ( 0.097)    Data  0.073 ( 0.073)    Loss 1.0040e+00 (1.0040e+00)    Acc@1  67.19 ( 67.19)   Acc@5  94.53 ( 94.53)
Epoch: [9][101/313]     Time  0.034 ( 0.037)    Data  0.023 ( 0.024)    Loss 1.1855e+00 (1.1148e+00)    Acc@1  54.69 ( 61.53)   Acc@5  91.41 ( 95.69)
Epoch: [9][201/313]     Time  0.037 ( 0.036)    Data  0.023 ( 0.024)    Loss 1.0286e+00 (1.1104e+00)    Acc@1  61.72 ( 61.99)   Acc@5 100.00 ( 95.78)
Epoch: [9][301/313]     Time  0.034 ( 0.037)    Data  0.024 ( 0.024)    Loss 1.1871e+00 (1.1312e+00)    Acc@1  60.16 ( 61.23)   Acc@5  95.31 ( 95.60)
train Loss: 1.1323 Acc: 0.6119
Epoch: [9][  1/313]     Time  0.047 ( 0.036)    Data  0.034 ( 0.024)    Loss 8.7318e-01 (1.1314e+00)    Acc@1  66.41 ( 61.21)   Acc@5  97.66 ( 95.57)
val Loss: 1.0838 Acc: 0.6336

Epoch 10/39
----------
Epoch: [10][  1/313]    Time  0.098 ( 0.098)    Data  0.069 ( 0.069)    Loss 9.8809e-01 (9.8809e-01)    Acc@1  61.72 ( 61.72)   Acc@5  95.31 ( 95.31)
Epoch: [10][101/313]    Time  0.035 ( 0.036)    Data  0.024 ( 0.024)    Loss 1.0541e+00 (1.1102e+00)    Acc@1  60.94 ( 61.94)   Acc@5  96.09 ( 95.50)
Epoch: [10][201/313]    Time  0.041 ( 0.036)    Data  0.023 ( 0.024)    Loss 1.3191e+00 (1.1207e+00)    Acc@1  57.81 ( 61.65)   Acc@5  92.97 ( 95.39)
Epoch: [10][301/313]    Time  0.046 ( 0.036)    Data  0.024 ( 0.023)    Loss 1.4897e+00 (1.1306e+00)    Acc@1  51.56 ( 61.53)   Acc@5  91.41 ( 95.48)
train Loss: 1.1305 Acc: 0.6154
Epoch: [10][  1/313]    Time  0.045 ( 0.037)    Data  0.035 ( 0.023)    Loss 1.2444e+00 (1.1308e+00)    Acc@1  57.03 ( 61.53)   Acc@5  95.31 ( 95.48)
val Loss: 1.0998 Acc: 0.6259

Epoch 11/39
----------
Epoch: [11][  1/313]    Time  0.092 ( 0.092)    Data  0.070 ( 0.070)    Loss 9.3046e-01 (9.3046e-01)    Acc@1  64.84 ( 64.84)   Acc@5  96.88 ( 96.88)
Epoch: [11][101/313]    Time  0.035 ( 0.036)    Data  0.024 ( 0.024)    Loss 1.2310e+00 (1.0860e+00)    Acc@1  57.03 ( 63.25)   Acc@5  93.75 ( 95.85)
Epoch: [11][201/313]    Time  0.035 ( 0.035)    Data  0.023 ( 0.023)    Loss 1.1618e+00 (1.1227e+00)    Acc@1  60.94 ( 62.04)   Acc@5  94.53 ( 95.41)
Epoch: [11][301/313]    Time  0.035 ( 0.036)    Data  0.025 ( 0.023)    Loss 1.1599e+00 (1.1558e+00)    Acc@1  62.50 ( 60.88)   Acc@5  93.75 ( 94.96)
train Loss: 1.1569 Acc: 0.6086
Epoch: [11][  1/313]    Time  0.047 ( 0.036)    Data  0.034 ( 0.024)    Loss 1.0971e+00 (1.1567e+00)    Acc@1  69.53 ( 60.89)   Acc@5  95.31 ( 94.98)
val Loss: 1.1016 Acc: 0.6268

Epoch 12/39
----------
Epoch: [12][  1/313]    Time  0.094 ( 0.094)    Data  0.071 ( 0.071)    Loss 9.4530e-01 (9.4530e-01)    Acc@1  68.75 ( 68.75)   Acc@5  97.66 ( 97.66)
Epoch: [12][101/313]    Time  0.034 ( 0.037)    Data  0.023 ( 0.024)    Loss 1.1410e+00 (1.1685e+00)    Acc@1  63.28 ( 60.29)   Acc@5  96.88 ( 95.07)
Epoch: [12][201/313]    Time  0.036 ( 0.036)    Data  0.024 ( 0.024)    Loss 1.1171e+00 (1.1831e+00)    Acc@1  63.28 ( 60.12)   Acc@5  94.53 ( 94.95)
Epoch: [12][301/313]    Time  0.069 ( 0.038)    Data  0.045 ( 0.025)    Loss 1.4135e+00 (1.1911e+00)    Acc@1  60.16 ( 59.91)   Acc@5  92.97 ( 94.78)
train Loss: 1.1903 Acc: 0.5992
Epoch: [12][  1/313]    Time  0.054 ( 0.039)    Data  0.041 ( 0.025)    Loss 1.2974e+00 (1.1907e+00)    Acc@1  56.25 ( 59.91)   Acc@5  96.88 ( 94.83)
val Loss: 1.1921 Acc: 0.5928

Epoch 13/39
----------
Epoch: [13][  1/313]    Time  0.107 ( 0.107)    Data  0.079 ( 0.079)    Loss 1.3793e+00 (1.3793e+00)    Acc@1  51.56 ( 51.56)   Acc@5  94.53 ( 94.53)
Epoch: [13][101/313]    Time  0.036 ( 0.036)    Data  0.025 ( 0.024)    Loss 1.2102e+00 (1.1197e+00)    Acc@1  60.16 ( 61.80)   Acc@5  93.75 ( 95.61)
Epoch: [13][201/313]    Time  0.036 ( 0.036)    Data  0.023 ( 0.024)    Loss 1.1569e+00 (1.1444e+00)    Acc@1  57.81 ( 61.31)   Acc@5  95.31 ( 95.29)
Epoch: [13][301/313]    Time  0.036 ( 0.038)    Data  0.025 ( 0.025)    Loss 1.1782e+00 (1.1524e+00)    Acc@1  67.19 ( 61.11)   Acc@5  92.97 ( 95.13)
train Loss: 1.1514 Acc: 0.6115
Epoch: [13][  1/313]    Time  0.049 ( 0.038)    Data  0.046 ( 0.025)    Loss 1.1087e+00 (1.1512e+00)    Acc@1  64.06 ( 61.16)   Acc@5  92.19 ( 95.11)
val Loss: 1.1096 Acc: 0.6263

Epoch 14/39
----------
Epoch: [14][  1/313]    Time  0.077 ( 0.077)    Data  0.073 ( 0.073)    Loss 1.0264e+00 (1.0264e+00)    Acc@1  66.41 ( 66.41)   Acc@5  94.53 ( 94.53)
Epoch: [14][101/313]    Time  0.029 ( 0.028)    Data  0.024 ( 0.024)    Loss 1.2030e+00 (1.1231e+00)    Acc@1  53.91 ( 62.62)   Acc@5  93.75 ( 95.37)
Epoch: [14][201/313]    Time  0.035 ( 0.031)    Data  0.023 ( 0.023)    Loss 1.1201e+00 (1.1349e+00)    Acc@1  65.62 ( 61.90)   Acc@5  97.66 ( 95.17)
Epoch: [14][301/313]    Time  0.035 ( 0.033)    Data  0.024 ( 0.024)    Loss 1.2040e+00 (1.1614e+00)    Acc@1  59.38 ( 60.98)   Acc@5  92.19 ( 94.89)
train Loss: 1.1659 Acc: 0.6086
Epoch: [14][  1/313]    Time  0.050 ( 0.033)    Data  0.038 ( 0.024)    Loss 9.9378e-01 (1.1654e+00)    Acc@1  64.84 ( 60.87)   Acc@5  98.44 ( 94.90)
val Loss: 1.1282 Acc: 0.6068

Epoch 15/39
----------
Epoch: [15][  1/313]    Time  0.139 ( 0.139)    Data  0.125 ( 0.125)    Loss 1.0059e+00 (1.0059e+00)    Acc@1  67.19 ( 67.19)   Acc@5  97.66 ( 97.66)
Epoch: [15][101/313]    Time  0.028 ( 0.034)    Data  0.024 ( 0.029)    Loss 1.1576e+00 (1.1260e+00)    Acc@1  65.62 ( 62.26)   Acc@5  89.84 ( 94.96)
Epoch: [15][201/313]    Time  0.041 ( 0.033)    Data  0.037 ( 0.027)    Loss 1.0946e+00 (1.1519e+00)    Acc@1  60.16 ( 61.53)   Acc@5  96.09 ( 94.88)
Epoch: [15][301/313]    Time  0.030 ( 0.034)    Data  0.025 ( 0.028)    Loss 1.3049e+00 (1.1648e+00)    Acc@1  59.38 ( 61.01)   Acc@5  96.09 ( 94.86)
train Loss: 1.1636 Acc: 0.6103
Epoch: [15][  1/313]    Time  0.044 ( 0.034)    Data  0.041 ( 0.028)    Loss 1.1344e+00 (1.1635e+00)    Acc@1  64.84 ( 61.04)   Acc@5  96.88 ( 94.86)
val Loss: 1.1298 Acc: 0.6154

Epoch 16/39
----------
Epoch: [16][  1/313]    Time  0.092 ( 0.092)    Data  0.076 ( 0.076)    Loss 8.6613e-01 (8.6613e-01)    Acc@1  69.53 ( 69.53)   Acc@5  98.44 ( 98.44)
Epoch: [16][101/313]    Time  0.039 ( 0.042)    Data  0.025 ( 0.029)    Loss 9.7102e-01 (1.1671e+00)    Acc@1  65.62 ( 60.75)   Acc@5  96.88 ( 94.93)
Epoch: [16][201/313]    Time  0.034 ( 0.042)    Data  0.024 ( 0.028)    Loss 1.2010e+00 (1.1843e+00)    Acc@1  61.72 ( 60.39)   Acc@5  94.53 ( 94.58)
Epoch: [16][301/313]    Time  0.034 ( 0.039)    Data  0.024 ( 0.026)    Loss 1.0936e+00 (1.1919e+00)    Acc@1  59.38 ( 60.04)   Acc@5  95.31 ( 94.57)
train Loss: 1.1886 Acc: 0.6010
Epoch: [16][  1/313]    Time  0.047 ( 0.039)    Data  0.034 ( 0.026)    Loss 1.0622e+00 (1.1882e+00)    Acc@1  64.06 ( 60.11)   Acc@5  95.31 ( 94.61)
val Loss: 1.0982 Acc: 0.6290

Epoch 17/39
----------
Epoch: [17][  1/313]    Time  0.085 ( 0.085)    Data  0.073 ( 0.073)    Loss 9.9484e-01 (9.9484e-01)    Acc@1  63.28 ( 63.28)   Acc@5  95.31 ( 95.31)
Epoch: [17][101/313]    Time  0.038 ( 0.046)    Data  0.023 ( 0.031)    Loss 1.0638e+00 (1.1392e+00)    Acc@1  59.38 ( 62.04)   Acc@5  98.44 ( 95.20)
Epoch: [17][201/313]    Time  0.042 ( 0.041)    Data  0.024 ( 0.027)    Loss 1.4633e+00 (1.1642e+00)    Acc@1  54.69 ( 61.27)   Acc@5  91.41 ( 94.79)
Epoch: [17][301/313]    Time  0.038 ( 0.040)    Data  0.023 ( 0.026)    Loss 1.1100e+00 (1.1864e+00)    Acc@1  60.16 ( 60.42)   Acc@5  93.75 ( 94.47)
train Loss: 1.1877 Acc: 0.6039
Epoch: [17][  1/313]    Time  0.041 ( 0.040)    Data  0.035 ( 0.026)    Loss 1.1763e+00 (1.1877e+00)    Acc@1  57.03 ( 60.38)   Acc@5  91.41 ( 94.47)
val Loss: 1.1391 Acc: 0.6216

Epoch 18/39
----------
Epoch: [18][  1/313]    Time  0.086 ( 0.086)    Data  0.073 ( 0.073)    Loss 1.0831e+00 (1.0831e+00)    Acc@1  65.62 ( 65.62)   Acc@5  92.97 ( 92.97)
Epoch: [18][101/313]    Time  0.037 ( 0.037)    Data  0.025 ( 0.024)    Loss 1.0683e+00 (1.1026e+00)    Acc@1  66.41 ( 63.31)   Acc@5  95.31 ( 95.20)
Epoch: [18][201/313]    Time  0.035 ( 0.037)    Data  0.023 ( 0.024)    Loss 1.1853e+00 (1.1392e+00)    Acc@1  64.06 ( 62.29)   Acc@5  94.53 ( 94.84)
Epoch: [18][301/313]    Time  0.032 ( 0.036)    Data  0.027 ( 0.024)    Loss 1.2353e+00 (1.1726e+00)    Acc@1  59.38 ( 61.09)   Acc@5  89.84 ( 94.54)
train Loss: 1.1738 Acc: 0.6102
Epoch: [18][  1/313]    Time  0.037 ( 0.036)    Data  0.035 ( 0.024)    Loss 1.2561e+00 (1.1741e+00)    Acc@1  57.81 ( 61.01)   Acc@5  92.97 ( 94.51)
val Loss: 1.1786 Acc: 0.5990

Epoch 19/39
----------
Epoch: [19][  1/313]    Time  0.081 ( 0.081)    Data  0.077 ( 0.077)    Loss 1.0417e+00 (1.0417e+00)    Acc@1  63.28 ( 63.28)   Acc@5  93.75 ( 93.75)
Epoch: [19][101/313]    Time  0.033 ( 0.031)    Data  0.022 ( 0.024)    Loss 1.1000e+00 (1.1661e+00)    Acc@1  63.28 ( 61.44)   Acc@5  94.53 ( 94.52)
Epoch: [19][201/313]    Time  0.057 ( 0.034)    Data  0.044 ( 0.024)    Loss 1.0113e+00 (1.1650e+00)    Acc@1  66.41 ( 61.12)   Acc@5  95.31 ( 94.56)
Epoch: [19][301/313]    Time  0.043 ( 0.035)    Data  0.029 ( 0.024)    Loss 9.3704e-01 (1.1688e+00)    Acc@1  66.41 ( 61.12)   Acc@5  97.66 ( 94.61)
train Loss: 1.1698 Acc: 0.6112
Epoch: [19][  1/313]    Time  0.048 ( 0.035)    Data  0.041 ( 0.024)    Loss 1.0570e+00 (1.1694e+00)    Acc@1  64.84 ( 61.13)   Acc@5  92.97 ( 94.59)
val Loss: 1.1518 Acc: 0.6182

Epoch 20/39
----------
Epoch: [20][  1/313]    Time  0.095 ( 0.095)    Data  0.072 ( 0.072)    Loss 1.1337e+00 (1.1337e+00)    Acc@1  57.03 ( 57.03)   Acc@5  93.75 ( 93.75)
Epoch: [20][101/313]    Time  0.028 ( 0.034)    Data  0.025 ( 0.025)    Loss 1.1372e+00 (1.1685e+00)    Acc@1  62.50 ( 60.65)   Acc@5  92.19 ( 94.65)
Epoch: [20][201/313]    Time  0.029 ( 0.031)    Data  0.024 ( 0.024)    Loss 1.0187e+00 (1.1639e+00)    Acc@1  66.41 ( 61.10)   Acc@5  97.66 ( 94.61)
Epoch: [20][301/313]    Time  0.036 ( 0.032)    Data  0.024 ( 0.024)    Loss 1.1312e+00 (1.1845e+00)    Acc@1  62.50 ( 60.62)   Acc@5  94.53 ( 94.34)
train Loss: 1.1862 Acc: 0.6060
Epoch: [20][  1/313]    Time  0.045 ( 0.033)    Data  0.034 ( 0.024)    Loss 1.1859e+00 (1.1862e+00)    Acc@1  58.59 ( 60.60)   Acc@5  96.09 ( 94.37)
val Loss: 1.2378 Acc: 0.5764

Epoch 21/39
----------
Epoch: [21][  1/313]    Time  0.094 ( 0.094)    Data  0.073 ( 0.073)    Loss 1.3129e+00 (1.3129e+00)    Acc@1  54.69 ( 54.69)   Acc@5  96.88 ( 96.88)
Epoch: [21][101/313]    Time  0.032 ( 0.038)    Data  0.022 ( 0.025)    Loss 1.1120e+00 (1.2157e+00)    Acc@1  64.06 ( 59.56)   Acc@5  96.09 ( 94.01)
Epoch: [21][201/313]    Time  0.035 ( 0.036)    Data  0.025 ( 0.024)    Loss 1.1225e+00 (1.2221e+00)    Acc@1  59.38 ( 59.39)   Acc@5  94.53 ( 94.01)
Epoch: [21][301/313]    Time  0.035 ( 0.036)    Data  0.024 ( 0.024)    Loss 1.1073e+00 (1.2031e+00)    Acc@1  62.50 ( 59.72)   Acc@5  96.09 ( 94.20)
train Loss: 1.2010 Acc: 0.5980
Epoch: [21][  1/313]    Time  0.049 ( 0.037)    Data  0.044 ( 0.024)    Loss 1.2093e+00 (1.2010e+00)    Acc@1  57.81 ( 59.80)   Acc@5  94.53 ( 94.28)
val Loss: 1.1138 Acc: 0.6152

Epoch 22/39
----------
Epoch: [22][  1/313]    Time  0.097 ( 0.097)    Data  0.073 ( 0.073)    Loss 1.1507e+00 (1.1507e+00)    Acc@1  57.81 ( 57.81)   Acc@5  92.97 ( 92.97)
Epoch: [22][101/313]    Time  0.035 ( 0.038)    Data  0.025 ( 0.024)    Loss 1.3807e+00 (1.1557e+00)    Acc@1  58.59 ( 61.77)   Acc@5  92.19 ( 95.00)
Epoch: [22][201/313]    Time  0.035 ( 0.036)    Data  0.023 ( 0.024)    Loss 1.2276e+00 (1.1739e+00)    Acc@1  60.94 ( 61.07)   Acc@5  93.75 ( 94.55)
Epoch: [22][301/313]    Time  0.035 ( 0.036)    Data  0.023 ( 0.024)    Loss 1.1511e+00 (1.1815e+00)    Acc@1  59.38 ( 60.85)   Acc@5  96.09 ( 94.44)
train Loss: 1.1803 Acc: 0.6088
Epoch: [22][  1/313]    Time  0.085 ( 0.037)    Data  0.041 ( 0.024)    Loss 1.1480e+00 (1.1802e+00)    Acc@1  62.50 ( 60.88)   Acc@5  92.97 ( 94.45)
val Loss: 1.1354 Acc: 0.6244

Epoch 23/39
----------
Epoch: [23][  1/313]    Time  0.078 ( 0.078)    Data  0.073 ( 0.073)    Loss 1.0882e+00 (1.0882e+00)    Acc@1  62.50 ( 62.50)   Acc@5  96.88 ( 96.88)
Epoch: [23][101/313]    Time  0.035 ( 0.037)    Data  0.024 ( 0.028)    Loss 9.8908e-01 (1.1284e+00)    Acc@1  67.97 ( 62.35)   Acc@5  95.31 ( 94.83)
Epoch: [23][201/313]    Time  0.035 ( 0.036)    Data  0.023 ( 0.026)    Loss 9.0740e-01 (1.1733e+00)    Acc@1  66.41 ( 61.06)   Acc@5  95.31 ( 94.40)
Epoch: [23][301/313]    Time  0.033 ( 0.036)    Data  0.026 ( 0.026)    Loss 9.9300e-01 (1.1726e+00)    Acc@1  65.62 ( 60.99)   Acc@5  94.53 ( 94.54)
train Loss: 1.1713 Acc: 0.6103
Epoch: [23][  1/313]    Time  0.038 ( 0.035)    Data  0.036 ( 0.026)    Loss 1.2576e+00 (1.1716e+00)    Acc@1  55.47 ( 61.01)   Acc@5  92.97 ( 94.53)
val Loss: 1.1701 Acc: 0.6064

Epoch 24/39
----------
Epoch: [24][  1/313]    Time  0.082 ( 0.082)    Data  0.071 ( 0.071)    Loss 1.4102e+00 (1.4102e+00)    Acc@1  55.47 ( 55.47)   Acc@5  89.84 ( 89.84)
Epoch: [24][101/313]    Time  0.028 ( 0.035)    Data  0.024 ( 0.025)    Loss 1.0797e+00 (1.1488e+00)    Acc@1  63.28 ( 61.81)   Acc@5  96.09 ( 95.24)
Epoch: [24][201/313]    Time  0.027 ( 0.032)    Data  0.024 ( 0.024)    Loss 1.2709e+00 (1.1633e+00)    Acc@1  60.94 ( 61.25)   Acc@5  96.09 ( 94.64)
Epoch: [24][301/313]    Time  0.036 ( 0.031)    Data  0.024 ( 0.024)    Loss 1.2892e+00 (1.1717e+00)    Acc@1  56.25 ( 60.90)   Acc@5  95.31 ( 94.48)
train Loss: 1.1719 Acc: 0.6093
Epoch: [24][  1/313]    Time  0.053 ( 0.032)    Data  0.038 ( 0.024)    Loss 1.4239e+00 (1.1727e+00)    Acc@1  52.34 ( 60.90)   Acc@5  92.97 ( 94.49)
val Loss: 1.1544 Acc: 0.6154

Epoch 25/39
----------
Epoch: [25][  1/313]    Time  0.075 ( 0.075)    Data  0.071 ( 0.071)    Loss 1.4453e+00 (1.4453e+00)    Acc@1  54.69 ( 54.69)   Acc@5  92.97 ( 92.97)
Epoch: [25][101/313]    Time  0.032 ( 0.032)    Data  0.025 ( 0.027)    Loss 1.1605e+00 (1.2115e+00)    Acc@1  61.72 ( 59.85)   Acc@5  96.09 ( 94.26)
Epoch: [25][201/313]    Time  0.034 ( 0.035)    Data  0.023 ( 0.026)    Loss 1.1985e+00 (1.2093e+00)    Acc@1  63.28 ( 60.22)   Acc@5  88.28 ( 94.13)
Epoch: [25][301/313]    Time  0.034 ( 0.035)    Data  0.022 ( 0.025)    Loss 1.0193e+00 (1.2097e+00)    Acc@1  64.84 ( 60.16)   Acc@5  92.97 ( 94.02)
train Loss: 1.2109 Acc: 0.6010
Epoch: [25][  1/313]    Time  0.074 ( 0.035)    Data  0.034 ( 0.025)    Loss 1.1701e+00 (1.2108e+00)    Acc@1  59.38 ( 60.10)   Acc@5  90.62 ( 94.04)
val Loss: 1.1937 Acc: 0.5972

Epoch 26/39
----------
Epoch: [26][  1/313]    Time  0.083 ( 0.083)    Data  0.071 ( 0.071)    Loss 1.0957e+00 (1.0957e+00)    Acc@1  60.16 ( 60.16)   Acc@5  95.31 ( 95.31)
Epoch: [26][101/313]    Time  0.036 ( 0.037)    Data  0.023 ( 0.024)    Loss 1.1448e+00 (1.1422e+00)    Acc@1  60.16 ( 61.88)   Acc@5  95.31 ( 94.72)
Epoch: [26][201/313]    Time  0.029 ( 0.036)    Data  0.025 ( 0.024)    Loss 1.1135e+00 (1.1564e+00)    Acc@1  62.50 ( 61.36)   Acc@5  98.44 ( 94.52)
Epoch: [26][301/313]    Time  0.032 ( 0.034)    Data  0.024 ( 0.024)    Loss 1.2942e+00 (1.1745e+00)    Acc@1  59.38 ( 60.87)   Acc@5  95.31 ( 94.37)
train Loss: 1.1793 Acc: 0.6074
Epoch: [26][  1/313]    Time  0.047 ( 0.035)    Data  0.041 ( 0.024)    Loss 1.2289e+00 (1.1794e+00)    Acc@1  57.81 ( 60.73)   Acc@5  92.19 ( 94.30)
val Loss: 1.2157 Acc: 0.6031

Epoch 27/39
----------
Epoch: [27][  1/313]    Time  0.076 ( 0.076)    Data  0.072 ( 0.072)    Loss 1.0466e+00 (1.0466e+00)    Acc@1  62.50 ( 62.50)   Acc@5  92.97 ( 92.97)
Epoch: [27][101/313]    Time  0.034 ( 0.034)    Data  0.023 ( 0.024)    Loss 1.0326e+00 (1.1816e+00)    Acc@1  67.97 ( 61.23)   Acc@5  96.09 ( 94.38)
Epoch: [27][201/313]    Time  0.027 ( 0.034)    Data  0.023 ( 0.024)    Loss 1.2192e+00 (1.1676e+00)    Acc@1  64.84 ( 61.63)   Acc@5  93.75 ( 94.45)
Epoch: [27][301/313]    Time  0.032 ( 0.032)    Data  0.024 ( 0.024)    Loss 1.2049e+00 (1.1834e+00)    Acc@1  58.59 ( 61.10)   Acc@5  93.75 ( 94.24)
train Loss: 1.1882 Acc: 0.6094
Epoch: [27][  1/313]    Time  0.039 ( 0.032)    Data  0.035 ( 0.024)    Loss 1.1860e+00 (1.1882e+00)    Acc@1  54.69 ( 60.92)   Acc@5  97.66 ( 94.21)
val Loss: 1.2414 Acc: 0.5782

Epoch 28/39
----------
Epoch: [28][  1/313]    Time  0.083 ( 0.083)    Data  0.071 ( 0.071)    Loss 1.1450e+00 (1.1450e+00)    Acc@1  58.59 ( 58.59)   Acc@5  92.97 ( 92.97)
Epoch: [28][101/313]    Time  0.041 ( 0.042)    Data  0.026 ( 0.027)    Loss 1.0036e+00 (1.1473e+00)    Acc@1  63.28 ( 61.90)   Acc@5  99.22 ( 94.84)
Epoch: [28][201/313]    Time  0.034 ( 0.036)    Data  0.024 ( 0.025)    Loss 1.0837e+00 (1.1700e+00)    Acc@1  60.94 ( 61.24)   Acc@5  95.31 ( 94.34)
Epoch: [28][301/313]    Time  0.034 ( 0.036)    Data  0.023 ( 0.025)    Loss 1.1381e+00 (1.1701e+00)    Acc@1  62.50 ( 61.32)   Acc@5  94.53 ( 94.46)
train Loss: 1.1710 Acc: 0.6131
Epoch: [28][  1/313]    Time  0.046 ( 0.036)    Data  0.035 ( 0.025)    Loss 9.6242e-01 (1.1703e+00)    Acc@1  71.09 ( 61.34)   Acc@5  93.75 ( 94.47)
val Loss: 1.1888 Acc: 0.6063

Epoch 29/39
----------
Epoch: [29][  1/313]    Time  0.092 ( 0.092)    Data  0.079 ( 0.079)    Loss 1.0561e+00 (1.0561e+00)    Acc@1  64.06 ( 64.06)   Acc@5  94.53 ( 94.53)
Epoch: [29][101/313]    Time  0.035 ( 0.037)    Data  0.022 ( 0.024)    Loss 1.4145e+00 (1.2298e+00)    Acc@1  50.78 ( 59.96)   Acc@5  92.19 ( 94.06)
Epoch: [29][201/313]    Time  0.035 ( 0.036)    Data  0.023 ( 0.024)    Loss 1.1868e+00 (1.2249e+00)    Acc@1  61.72 ( 59.99)   Acc@5  93.75 ( 94.06)
Epoch: [29][301/313]    Time  0.035 ( 0.036)    Data  0.023 ( 0.023)    Loss 1.1291e+00 (1.2125e+00)    Acc@1  64.84 ( 60.32)   Acc@5  94.53 ( 94.19)
train Loss: 1.2085 Acc: 0.6041
Epoch: [29][  1/313]    Time  0.040 ( 0.036)    Data  0.034 ( 0.023)    Loss 1.2793e+00 (1.2088e+00)    Acc@1  59.38 ( 60.41)   Acc@5  94.53 ( 94.23)
val Loss: 1.1818 Acc: 0.6046

Epoch 30/39
----------
Epoch: [30][  1/313]    Time  0.084 ( 0.084)    Data  0.071 ( 0.071)    Loss 1.2371e+00 (1.2371e+00)    Acc@1  59.38 ( 59.38)   Acc@5  94.53 ( 94.53)
Epoch: [30][101/313]    Time  0.036 ( 0.036)    Data  0.024 ( 0.024)    Loss 1.1838e+00 (1.1610e+00)    Acc@1  54.69 ( 61.91)   Acc@5  91.41 ( 94.86)
Epoch: [30][201/313]    Time  0.035 ( 0.037)    Data  0.023 ( 0.024)    Loss 1.2736e+00 (1.1730e+00)    Acc@1  53.91 ( 61.45)   Acc@5  93.75 ( 94.66)
Epoch: [30][301/313]    Time  0.035 ( 0.036)    Data  0.023 ( 0.023)    Loss 1.2382e+00 (1.1808e+00)    Acc@1  64.06 ( 60.98)   Acc@5  94.53 ( 94.60)
train Loss: 1.1821 Acc: 0.6098
Epoch: [30][  1/313]    Time  0.042 ( 0.036)    Data  0.036 ( 0.024)    Loss 1.3727e+00 (1.1827e+00)    Acc@1  53.91 ( 60.95)   Acc@5  91.41 ( 94.57)
val Loss: 1.2463 Acc: 0.5807

Epoch 31/39
----------
Epoch: [31][  1/313]    Time  0.100 ( 0.100)    Data  0.071 ( 0.071)    Loss 1.1529e+00 (1.1529e+00)    Acc@1  53.91 ( 53.91)   Acc@5  97.66 ( 97.66)
Epoch: [31][101/313]    Time  0.033 ( 0.037)    Data  0.023 ( 0.024)    Loss 9.9639e-01 (1.1555e+00)    Acc@1  68.75 ( 61.84)   Acc@5  94.53 ( 94.52)
Epoch: [31][201/313]    Time  0.035 ( 0.036)    Data  0.023 ( 0.024)    Loss 1.0644e+00 (1.1485e+00)    Acc@1  61.72 ( 62.20)   Acc@5  96.88 ( 94.67)
Epoch: [31][301/313]    Time  0.035 ( 0.036)    Data  0.023 ( 0.023)    Loss 1.0708e+00 (1.1630e+00)    Acc@1  62.50 ( 61.53)   Acc@5  96.88 ( 94.58)
train Loss: 1.1639 Acc: 0.6156
Epoch: [31][  1/313]    Time  0.047 ( 0.036)    Data  0.036 ( 0.023)    Loss 1.1085e+00 (1.1638e+00)    Acc@1  57.81 ( 61.54)   Acc@5  93.75 ( 94.55)
val Loss: 1.0947 Acc: 0.6336

Epoch 32/39
----------
Epoch: [32][  1/313]    Time  0.094 ( 0.094)    Data  0.071 ( 0.071)    Loss 9.6041e-01 (9.6041e-01)    Acc@1  71.88 ( 71.88)   Acc@5  95.31 ( 95.31)
Epoch: [32][101/313]    Time  0.038 ( 0.036)    Data  0.032 ( 0.025)    Loss 1.2283e+00 (1.1451e+00)    Acc@1  59.38 ( 62.70)   Acc@5  95.31 ( 94.62)
Epoch: [32][201/313]    Time  0.027 ( 0.032)    Data  0.022 ( 0.024)    Loss 1.2101e+00 (1.1679e+00)    Acc@1  67.19 ( 61.57)   Acc@5  94.53 ( 94.50)
Epoch: [32][301/313]    Time  0.034 ( 0.032)    Data  0.022 ( 0.024)    Loss 1.0746e+00 (1.1656e+00)    Acc@1  63.28 ( 61.60)   Acc@5  96.88 ( 94.60)
train Loss: 1.1659 Acc: 0.6158
Epoch: [32][  1/313]    Time  0.040 ( 0.032)    Data  0.034 ( 0.024)    Loss 1.0417e+00 (1.1655e+00)    Acc@1  64.06 ( 61.58)   Acc@5  93.75 ( 94.58)
val Loss: 1.1934 Acc: 0.6019

Epoch 33/39
----------
Epoch: [33][  1/313]    Time  0.097 ( 0.097)    Data  0.074 ( 0.074)    Loss 1.2088e+00 (1.2088e+00)    Acc@1  60.16 ( 60.16)   Acc@5  92.19 ( 92.19)
Epoch: [33][101/313]    Time  0.034 ( 0.036)    Data  0.023 ( 0.024)    Loss 1.2221e+00 (1.1590e+00)    Acc@1  53.91 ( 61.99)   Acc@5  95.31 ( 94.10)
Epoch: [33][201/313]    Time  0.035 ( 0.036)    Data  0.023 ( 0.024)    Loss 1.3197e+00 (1.1632e+00)    Acc@1  63.28 ( 61.63)   Acc@5  92.97 ( 94.47)
Epoch: [33][301/313]    Time  0.034 ( 0.036)    Data  0.023 ( 0.024)    Loss 1.1344e+00 (1.1716e+00)    Acc@1  64.84 ( 61.32)   Acc@5  94.53 ( 94.33)
train Loss: 1.1727 Acc: 0.6130
Epoch: [33][  1/313]    Time  0.042 ( 0.036)    Data  0.034 ( 0.024)    Loss 1.1668e+00 (1.1727e+00)    Acc@1  58.59 ( 61.29)   Acc@5  96.09 ( 94.35)
val Loss: 1.1584 Acc: 0.6204

Epoch 34/39
----------
Epoch: [34][  1/313]    Time  0.094 ( 0.094)    Data  0.071 ( 0.071)    Loss 1.2138e+00 (1.2138e+00)    Acc@1  65.62 ( 65.62)   Acc@5  95.31 ( 95.31)
Epoch: [34][101/313]    Time  0.039 ( 0.038)    Data  0.026 ( 0.024)    Loss 9.7504e-01 (1.1694e+00)    Acc@1  67.19 ( 60.93)   Acc@5  94.53 ( 94.76)
Epoch: [34][201/313]    Time  0.035 ( 0.036)    Data  0.023 ( 0.024)    Loss 1.3067e+00 (1.1768e+00)    Acc@1  60.94 ( 60.97)   Acc@5  93.75 ( 94.45)
Epoch: [34][301/313]    Time  0.035 ( 0.036)    Data  0.023 ( 0.024)    Loss 1.0417e+00 (1.1845e+00)    Acc@1  64.84 ( 60.80)   Acc@5  95.31 ( 94.37)
train Loss: 1.1862 Acc: 0.6075
Epoch: [34][  1/313]    Time  0.048 ( 0.036)    Data  0.035 ( 0.024)    Loss 1.1007e+00 (1.1859e+00)    Acc@1  63.28 ( 60.76)   Acc@5  96.09 ( 94.36)
val Loss: 1.2703 Acc: 0.5722

Epoch 35/39
----------
Epoch: [35][  1/313]    Time  0.094 ( 0.094)    Data  0.072 ( 0.072)    Loss 1.1161e+00 (1.1161e+00)    Acc@1  61.72 ( 61.72)   Acc@5  94.53 ( 94.53)
Epoch: [35][101/313]    Time  0.036 ( 0.038)    Data  0.028 ( 0.028)    Loss 9.6651e-01 (1.1730e+00)    Acc@1  64.06 ( 60.85)   Acc@5  98.44 ( 94.44)
Epoch: [35][201/313]    Time  0.030 ( 0.034)    Data  0.024 ( 0.027)    Loss 1.4603e+00 (1.1884e+00)    Acc@1  55.47 ( 60.78)   Acc@5  92.97 ( 94.33)
Epoch: [35][301/313]    Time  0.038 ( 0.035)    Data  0.025 ( 0.026)    Loss 1.3826e+00 (1.1934e+00)    Acc@1  50.78 ( 60.66)   Acc@5  91.41 ( 94.11)
train Loss: 1.1920 Acc: 0.6068
Epoch: [35][  1/313]    Time  0.061 ( 0.035)    Data  0.043 ( 0.026)    Loss 1.2454e+00 (1.1922e+00)    Acc@1  57.03 ( 60.67)   Acc@5  94.53 ( 94.13)
val Loss: 1.1715 Acc: 0.6090

Epoch 36/39
----------
Epoch: [36][  1/313]    Time  0.087 ( 0.087)    Data  0.083 ( 0.083)    Loss 1.1491e+00 (1.1491e+00)    Acc@1  66.41 ( 66.41)   Acc@5  93.75 ( 93.75)
Epoch: [36][101/313]    Time  0.031 ( 0.031)    Data  0.026 ( 0.026)    Loss 1.1248e+00 (1.1302e+00)    Acc@1  61.72 ( 62.79)   Acc@5  96.09 ( 94.67)
Epoch: [36][201/313]    Time  0.029 ( 0.030)    Data  0.023 ( 0.025)    Loss 1.1532e+00 (1.1639e+00)    Acc@1  60.16 ( 61.91)   Acc@5  96.88 ( 94.29)
Epoch: [36][301/313]    Time  0.033 ( 0.032)    Data  0.023 ( 0.025)    Loss 1.0884e+00 (1.1731e+00)    Acc@1  57.03 ( 61.61)   Acc@5  96.88 ( 94.17)
train Loss: 1.1772 Acc: 0.6139
Epoch: [36][  1/313]    Time  0.054 ( 0.033)    Data  0.048 ( 0.025)    Loss 1.4602e+00 (1.1781e+00)    Acc@1  50.78 ( 61.36)   Acc@5  92.19 ( 94.12)
val Loss: 1.3099 Acc: 0.5548

Epoch 37/39
----------
Epoch: [37][  1/313]    Time  0.115 ( 0.115)    Data  0.097 ( 0.097)    Loss 1.5187e+00 (1.5187e+00)    Acc@1  55.47 ( 55.47)   Acc@5  92.97 ( 92.97)
Epoch: [37][101/313]    Time  0.031 ( 0.039)    Data  0.025 ( 0.033)    Loss 1.0384e+00 (1.1984e+00)    Acc@1  64.84 ( 60.68)   Acc@5  95.31 ( 94.26)
Epoch: [37][201/313]    Time  0.036 ( 0.037)    Data  0.024 ( 0.030)    Loss 1.3343e+00 (1.1854e+00)    Acc@1  54.69 ( 61.17)   Acc@5  92.97 ( 94.37)
Epoch: [37][301/313]    Time  0.028 ( 0.037)    Data  0.024 ( 0.029)    Loss 1.4026e+00 (1.2047e+00)    Acc@1  57.81 ( 60.33)   Acc@5  90.62 ( 94.18)
train Loss: 1.2080 Acc: 0.6023
Epoch: [37][  1/313]    Time  0.040 ( 0.036)    Data  0.036 ( 0.029)    Loss 1.3349e+00 (1.2084e+00)    Acc@1  59.38 ( 60.22)   Acc@5  92.97 ( 94.12)
val Loss: 1.2136 Acc: 0.6021

Epoch 38/39
----------
Epoch: [38][  1/313]    Time  0.081 ( 0.081)    Data  0.075 ( 0.075)    Loss 1.2053e+00 (1.2053e+00)    Acc@1  59.38 ( 59.38)   Acc@5  92.97 ( 92.97)
Epoch: [38][101/313]    Time  0.036 ( 0.036)    Data  0.026 ( 0.026)    Loss 1.0168e+00 (1.2201e+00)    Acc@1  67.19 ( 60.23)   Acc@5  93.75 ( 93.76)
Epoch: [38][201/313]    Time  0.028 ( 0.034)    Data  0.024 ( 0.026)    Loss 1.0927e+00 (1.2123e+00)    Acc@1  59.38 ( 60.33)   Acc@5  96.88 ( 93.92)
Epoch: [38][301/313]    Time  0.029 ( 0.032)    Data  0.023 ( 0.025)    Loss 1.2411e+00 (1.2279e+00)    Acc@1  60.16 ( 59.81)   Acc@5  92.97 ( 93.79)
train Loss: 1.2293 Acc: 0.5976
Epoch: [38][  1/313]    Time  0.040 ( 0.032)    Data  0.035 ( 0.025)    Loss 1.1956e+00 (1.2292e+00)    Acc@1  61.72 ( 59.76)   Acc@5  89.84 ( 93.75)
val Loss: 1.2198 Acc: 0.5951

Epoch 39/39
----------
Epoch: [39][  1/313]    Time  0.084 ( 0.084)    Data  0.071 ( 0.071)    Loss 1.1818e+00 (1.1818e+00)    Acc@1  57.03 ( 57.03)   Acc@5  95.31 ( 95.31)
Epoch: [39][101/313]    Time  0.028 ( 0.036)    Data  0.023 ( 0.025)    Loss 9.5155e-01 (1.2387e+00)    Acc@1  64.84 ( 59.75)   Acc@5  93.75 ( 93.27)
Epoch: [39][201/313]    Time  0.034 ( 0.034)    Data  0.024 ( 0.025)    Loss 1.4698e+00 (1.2225e+00)    Acc@1  55.47 ( 59.99)   Acc@5  96.88 ( 93.59)
Epoch: [39][301/313]    Time  0.027 ( 0.034)    Data  0.023 ( 0.025)    Loss 1.2111e+00 (1.2219e+00)    Acc@1  63.28 ( 60.08)   Acc@5  96.88 ( 93.69)
train Loss: 1.2237 Acc: 0.6006
Epoch: [39][  1/313]    Time  0.042 ( 0.034)    Data  0.039 ( 0.025)    Loss 1.1445e+00 (1.2234e+00)    Acc@1  64.84 ( 60.08)   Acc@5  92.19 ( 93.69)
val Loss: 1.2590 Acc: 0.5849

Training complete in 9m 7s
Best val Acc: 0.683900
Test Loss: 0.183117

Test Accuracy of airplane: 74% (732/989)
Test Accuracy of automobile: 83% (860/1026)
Test Accuracy of  bird: 43% (453/1043)
Test Accuracy of   cat: 34% (352/1029)
Test Accuracy of  deer: 65% (661/1013)
Test Accuracy of   dog: 73% (700/952)
Test Accuracy of  frog: 70% (683/967)
Test Accuracy of horse: 83% (843/1010)
Test Accuracy of  ship: 82% (788/959)
Test Accuracy of truck: 75% (767/1012)

Test Accuracy (Overall): 68% (6839/10000)