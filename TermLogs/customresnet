PS C:\mygit\MultiModalClassifier> python .\TorchClassifier\myTorchTrainer.py --data_name 'CIFAR10' --data_type 'torchvisiondataset' --data_path "C:\mygit\TrainingData\data\" --model_name 'customresnet' --learningratename 'ConstantLR' --optimizer 'SGD'
2.2.1
Torch Version:  2.2.1
Torchvision Version:  0.17.1
Output path: ./outputs/CIFAR10_customresnet_0910
Num GPUs: 1
0
NVIDIA GeForce RTX 3070 Ti
True
Files already downloaded and verified
Files already downloaded and verified
Number of training examples: 50000
Number of testing examples: 10000
C:\Users\Kenneth\anaconda3\envs\gpuenv\Lib\site-packages\torchvision\models\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
C:\Users\Kenneth\anaconda3\envs\gpuenv\Lib\site-packages\torchvision\models\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Downloading: "https://download.pytorch.org/models/resnet50-0676ba61.pth" to /data/cmpe249-fa23/torchhome/hub\checkpoints\resnet50-0676ba61.pth
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 97.8M/97.8M [00:01<00:00, 71.7MB/s]
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=2048, out_features=1000, bias=True)
)
The model has 23,528,522 trainable parameters
========================================================================================================================
Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable
========================================================================================================================
ResNet (ResNet)                          [128, 3, 224, 224]   [128, 10]            --                   True
├─Conv2d (conv1)                         [128, 3, 224, 224]   [128, 64, 112, 112]  9,408                True
├─BatchNorm2d (bn1)                      [128, 64, 112, 112]  [128, 64, 112, 112]  128                  True
├─ReLU (relu)                            [128, 64, 112, 112]  [128, 64, 112, 112]  --                   --
├─MaxPool2d (maxpool)                    [128, 64, 112, 112]  [128, 64, 56, 56]    --                   --
├─Sequential (layer1)                    [128, 64, 56, 56]    [128, 256, 56, 56]   --                   True
│    └─Bottleneck (0)                    [128, 64, 56, 56]    [128, 256, 56, 56]   --                   True
│    │    └─Conv2d (conv1)               [128, 64, 56, 56]    [128, 64, 56, 56]    4,096                True
│    │    └─BatchNorm2d (bn1)            [128, 64, 56, 56]    [128, 64, 56, 56]    128                  True
│    │    └─ReLU (relu)                  [128, 64, 56, 56]    [128, 64, 56, 56]    --                   --
│    │    └─Conv2d (conv2)               [128, 64, 56, 56]    [128, 64, 56, 56]    36,864               True
│    │    └─BatchNorm2d (bn2)            [128, 64, 56, 56]    [128, 64, 56, 56]    128                  True
│    │    └─ReLU (relu)                  [128, 64, 56, 56]    [128, 64, 56, 56]    --                   --
│    │    └─Conv2d (conv3)               [128, 64, 56, 56]    [128, 256, 56, 56]   16,384               True
│    │    └─BatchNorm2d (bn3)            [128, 256, 56, 56]   [128, 256, 56, 56]   512                  True
│    │    └─Sequential (downsample)      [128, 64, 56, 56]    [128, 256, 56, 56]   16,896               True
│    │    └─ReLU (relu)                  [128, 256, 56, 56]   [128, 256, 56, 56]   --                   --
│    └─Bottleneck (1)                    [128, 256, 56, 56]   [128, 256, 56, 56]   --                   True
│    │    └─Conv2d (conv1)               [128, 256, 56, 56]   [128, 64, 56, 56]    16,384               True
│    │    └─BatchNorm2d (bn1)            [128, 64, 56, 56]    [128, 64, 56, 56]    128                  True
│    │    └─ReLU (relu)                  [128, 64, 56, 56]    [128, 64, 56, 56]    --                   --
│    │    └─Conv2d (conv2)               [128, 64, 56, 56]    [128, 64, 56, 56]    36,864               True
│    │    └─BatchNorm2d (bn2)            [128, 64, 56, 56]    [128, 64, 56, 56]    128                  True
│    │    └─ReLU (relu)                  [128, 64, 56, 56]    [128, 64, 56, 56]    --                   --
│    │    └─Conv2d (conv3)               [128, 64, 56, 56]    [128, 256, 56, 56]   16,384               True
│    │    └─BatchNorm2d (bn3)            [128, 256, 56, 56]   [128, 256, 56, 56]   512                  True
│    │    └─ReLU (relu)                  [128, 256, 56, 56]   [128, 256, 56, 56]   --                   --
│    └─Bottleneck (2)                    [128, 256, 56, 56]   [128, 256, 56, 56]   --                   True
│    │    └─Conv2d (conv1)               [128, 256, 56, 56]   [128, 64, 56, 56]    16,384               True
│    │    └─BatchNorm2d (bn1)            [128, 64, 56, 56]    [128, 64, 56, 56]    128                  True
│    │    └─ReLU (relu)                  [128, 64, 56, 56]    [128, 64, 56, 56]    --                   --
│    │    └─Conv2d (conv2)               [128, 64, 56, 56]    [128, 64, 56, 56]    36,864               True
│    │    └─BatchNorm2d (bn2)            [128, 64, 56, 56]    [128, 64, 56, 56]    128                  True
│    │    └─ReLU (relu)                  [128, 64, 56, 56]    [128, 64, 56, 56]    --                   --
│    │    └─Conv2d (conv3)               [128, 64, 56, 56]    [128, 256, 56, 56]   16,384               True
│    │    └─BatchNorm2d (bn3)            [128, 256, 56, 56]   [128, 256, 56, 56]   512                  True
│    │    └─ReLU (relu)                  [128, 256, 56, 56]   [128, 256, 56, 56]   --                   --
├─Sequential (layer2)                    [128, 256, 56, 56]   [128, 512, 28, 28]   --                   True
│    └─Bottleneck (0)                    [128, 256, 56, 56]   [128, 512, 28, 28]   --                   True
│    │    └─Conv2d (conv1)               [128, 256, 56, 56]   [128, 128, 56, 56]   32,768               True
│    │    └─BatchNorm2d (bn1)            [128, 128, 56, 56]   [128, 128, 56, 56]   256                  True
│    │    └─ReLU (relu)                  [128, 128, 56, 56]   [128, 128, 56, 56]   --                   --
│    │    └─Conv2d (conv2)               [128, 128, 56, 56]   [128, 128, 28, 28]   147,456              True
│    │    └─BatchNorm2d (bn2)            [128, 128, 28, 28]   [128, 128, 28, 28]   256                  True
│    │    └─ReLU (relu)                  [128, 128, 28, 28]   [128, 128, 28, 28]   --                   --
│    │    └─Conv2d (conv3)               [128, 128, 28, 28]   [128, 512, 28, 28]   65,536               True
│    │    └─BatchNorm2d (bn3)            [128, 512, 28, 28]   [128, 512, 28, 28]   1,024                True
│    │    └─Sequential (downsample)      [128, 256, 56, 56]   [128, 512, 28, 28]   132,096              True
│    │    └─ReLU (relu)                  [128, 512, 28, 28]   [128, 512, 28, 28]   --                   --
│    └─Bottleneck (1)                    [128, 512, 28, 28]   [128, 512, 28, 28]   --                   True
│    │    └─Conv2d (conv1)               [128, 512, 28, 28]   [128, 128, 28, 28]   65,536               True
│    │    └─BatchNorm2d (bn1)            [128, 128, 28, 28]   [128, 128, 28, 28]   256                  True
│    │    └─ReLU (relu)                  [128, 128, 28, 28]   [128, 128, 28, 28]   --                   --
│    │    └─Conv2d (conv2)               [128, 128, 28, 28]   [128, 128, 28, 28]   147,456              True
│    │    └─BatchNorm2d (bn2)            [128, 128, 28, 28]   [128, 128, 28, 28]   256                  True
│    │    └─ReLU (relu)                  [128, 128, 28, 28]   [128, 128, 28, 28]   --                   --
│    │    └─Conv2d (conv3)               [128, 128, 28, 28]   [128, 512, 28, 28]   65,536               True
│    │    └─BatchNorm2d (bn3)            [128, 512, 28, 28]   [128, 512, 28, 28]   1,024                True
│    │    └─ReLU (relu)                  [128, 512, 28, 28]   [128, 512, 28, 28]   --                   --
│    └─Bottleneck (2)                    [128, 512, 28, 28]   [128, 512, 28, 28]   --                   True
│    │    └─Conv2d (conv1)               [128, 512, 28, 28]   [128, 128, 28, 28]   65,536               True
│    │    └─BatchNorm2d (bn1)            [128, 128, 28, 28]   [128, 128, 28, 28]   256                  True
│    │    └─ReLU (relu)                  [128, 128, 28, 28]   [128, 128, 28, 28]   --                   --
│    │    └─Conv2d (conv2)               [128, 128, 28, 28]   [128, 128, 28, 28]   147,456              True
│    │    └─BatchNorm2d (bn2)            [128, 128, 28, 28]   [128, 128, 28, 28]   256                  True
│    │    └─ReLU (relu)                  [128, 128, 28, 28]   [128, 128, 28, 28]   --                   --
│    │    └─Conv2d (conv3)               [128, 128, 28, 28]   [128, 512, 28, 28]   65,536               True
│    │    └─BatchNorm2d (bn3)            [128, 512, 28, 28]   [128, 512, 28, 28]   1,024                True
│    │    └─ReLU (relu)                  [128, 512, 28, 28]   [128, 512, 28, 28]   --                   --
│    └─Bottleneck (3)                    [128, 512, 28, 28]   [128, 512, 28, 28]   --                   True
│    │    └─Conv2d (conv1)               [128, 512, 28, 28]   [128, 128, 28, 28]   65,536               True
│    │    └─BatchNorm2d (bn1)            [128, 128, 28, 28]   [128, 128, 28, 28]   256                  True
│    │    └─ReLU (relu)                  [128, 128, 28, 28]   [128, 128, 28, 28]   --                   --
│    │    └─Conv2d (conv2)               [128, 128, 28, 28]   [128, 128, 28, 28]   147,456              True
│    │    └─BatchNorm2d (bn2)            [128, 128, 28, 28]   [128, 128, 28, 28]   256                  True
│    │    └─ReLU (relu)                  [128, 128, 28, 28]   [128, 128, 28, 28]   --                   --
│    │    └─Conv2d (conv3)               [128, 128, 28, 28]   [128, 512, 28, 28]   65,536               True
│    │    └─BatchNorm2d (bn3)            [128, 512, 28, 28]   [128, 512, 28, 28]   1,024                True
│    │    └─ReLU (relu)                  [128, 512, 28, 28]   [128, 512, 28, 28]   --                   --
├─Sequential (layer3)                    [128, 512, 28, 28]   [128, 1024, 14, 14]  --                   True
│    └─Bottleneck (0)                    [128, 512, 28, 28]   [128, 1024, 14, 14]  --                   True
│    │    └─Conv2d (conv1)               [128, 512, 28, 28]   [128, 256, 28, 28]   131,072              True
│    │    └─BatchNorm2d (bn1)            [128, 256, 28, 28]   [128, 256, 28, 28]   512                  True
│    │    └─ReLU (relu)                  [128, 256, 28, 28]   [128, 256, 28, 28]   --                   --
│    │    └─Conv2d (conv2)               [128, 256, 28, 28]   [128, 256, 14, 14]   589,824              True
│    │    └─BatchNorm2d (bn2)            [128, 256, 14, 14]   [128, 256, 14, 14]   512                  True
│    │    └─ReLU (relu)                  [128, 256, 14, 14]   [128, 256, 14, 14]   --                   --
│    │    └─Conv2d (conv3)               [128, 256, 14, 14]   [128, 1024, 14, 14]  262,144              True
│    │    └─BatchNorm2d (bn3)            [128, 1024, 14, 14]  [128, 1024, 14, 14]  2,048                True
│    │    └─Sequential (downsample)      [128, 512, 28, 28]   [128, 1024, 14, 14]  526,336              True
│    │    └─ReLU (relu)                  [128, 1024, 14, 14]  [128, 1024, 14, 14]  --                   --
│    └─Bottleneck (1)                    [128, 1024, 14, 14]  [128, 1024, 14, 14]  --                   True
│    │    └─Conv2d (conv1)               [128, 1024, 14, 14]  [128, 256, 14, 14]   262,144              True
│    │    └─BatchNorm2d (bn1)            [128, 256, 14, 14]   [128, 256, 14, 14]   512                  True
│    │    └─ReLU (relu)                  [128, 256, 14, 14]   [128, 256, 14, 14]   --                   --
│    │    └─Conv2d (conv2)               [128, 256, 14, 14]   [128, 256, 14, 14]   589,824              True
│    │    └─BatchNorm2d (bn2)            [128, 256, 14, 14]   [128, 256, 14, 14]   512                  True
│    │    └─ReLU (relu)                  [128, 256, 14, 14]   [128, 256, 14, 14]   --                   --
│    │    └─Conv2d (conv3)               [128, 256, 14, 14]   [128, 1024, 14, 14]  262,144              True
│    │    └─BatchNorm2d (bn3)            [128, 1024, 14, 14]  [128, 1024, 14, 14]  2,048                True
│    │    └─ReLU (relu)                  [128, 1024, 14, 14]  [128, 1024, 14, 14]  --                   --
│    └─Bottleneck (2)                    [128, 1024, 14, 14]  [128, 1024, 14, 14]  --                   True
│    │    └─Conv2d (conv1)               [128, 1024, 14, 14]  [128, 256, 14, 14]   262,144              True
│    │    └─BatchNorm2d (bn1)            [128, 256, 14, 14]   [128, 256, 14, 14]   512                  True
│    │    └─ReLU (relu)                  [128, 256, 14, 14]   [128, 256, 14, 14]   --                   --
│    │    └─Conv2d (conv2)               [128, 256, 14, 14]   [128, 256, 14, 14]   589,824              True
│    │    └─BatchNorm2d (bn2)            [128, 256, 14, 14]   [128, 256, 14, 14]   512                  True
│    │    └─ReLU (relu)                  [128, 256, 14, 14]   [128, 256, 14, 14]   --                   --
│    │    └─Conv2d (conv3)               [128, 256, 14, 14]   [128, 1024, 14, 14]  262,144              True
│    │    └─BatchNorm2d (bn3)            [128, 1024, 14, 14]  [128, 1024, 14, 14]  2,048                True
│    │    └─ReLU (relu)                  [128, 1024, 14, 14]  [128, 1024, 14, 14]  --                   --
│    └─Bottleneck (3)                    [128, 1024, 14, 14]  [128, 1024, 14, 14]  --                   True
│    │    └─Conv2d (conv1)               [128, 1024, 14, 14]  [128, 256, 14, 14]   262,144              True
│    │    └─BatchNorm2d (bn1)            [128, 256, 14, 14]   [128, 256, 14, 14]   512                  True
│    │    └─ReLU (relu)                  [128, 256, 14, 14]   [128, 256, 14, 14]   --                   --
│    │    └─Conv2d (conv2)               [128, 256, 14, 14]   [128, 256, 14, 14]   589,824              True
│    │    └─BatchNorm2d (bn2)            [128, 256, 14, 14]   [128, 256, 14, 14]   512                  True
│    │    └─ReLU (relu)                  [128, 256, 14, 14]   [128, 256, 14, 14]   --                   --
│    │    └─Conv2d (conv3)               [128, 256, 14, 14]   [128, 1024, 14, 14]  262,144              True
│    │    └─BatchNorm2d (bn3)            [128, 1024, 14, 14]  [128, 1024, 14, 14]  2,048                True
│    │    └─ReLU (relu)                  [128, 1024, 14, 14]  [128, 1024, 14, 14]  --                   --
│    └─Bottleneck (4)                    [128, 1024, 14, 14]  [128, 1024, 14, 14]  --                   True
│    │    └─Conv2d (conv1)               [128, 1024, 14, 14]  [128, 256, 14, 14]   262,144              True
│    │    └─BatchNorm2d (bn1)            [128, 256, 14, 14]   [128, 256, 14, 14]   512                  True
│    │    └─ReLU (relu)                  [128, 256, 14, 14]   [128, 256, 14, 14]   --                   --
│    │    └─Conv2d (conv2)               [128, 256, 14, 14]   [128, 256, 14, 14]   589,824              True
│    │    └─BatchNorm2d (bn2)            [128, 256, 14, 14]   [128, 256, 14, 14]   512                  True
│    │    └─ReLU (relu)                  [128, 256, 14, 14]   [128, 256, 14, 14]   --                   --
│    │    └─Conv2d (conv3)               [128, 256, 14, 14]   [128, 1024, 14, 14]  262,144              True
│    │    └─BatchNorm2d (bn3)            [128, 1024, 14, 14]  [128, 1024, 14, 14]  2,048                True
│    │    └─ReLU (relu)                  [128, 1024, 14, 14]  [128, 1024, 14, 14]  --                   --
│    └─Bottleneck (5)                    [128, 1024, 14, 14]  [128, 1024, 14, 14]  --                   True
│    │    └─Conv2d (conv1)               [128, 1024, 14, 14]  [128, 256, 14, 14]   262,144              True
│    │    └─BatchNorm2d (bn1)            [128, 256, 14, 14]   [128, 256, 14, 14]   512                  True
│    │    └─ReLU (relu)                  [128, 256, 14, 14]   [128, 256, 14, 14]   --                   --
│    │    └─Conv2d (conv2)               [128, 256, 14, 14]   [128, 256, 14, 14]   589,824              True
│    │    └─BatchNorm2d (bn2)            [128, 256, 14, 14]   [128, 256, 14, 14]   512                  True
│    │    └─ReLU (relu)                  [128, 256, 14, 14]   [128, 256, 14, 14]   --                   --
│    │    └─Conv2d (conv3)               [128, 256, 14, 14]   [128, 1024, 14, 14]  262,144              True
│    │    └─BatchNorm2d (bn3)            [128, 1024, 14, 14]  [128, 1024, 14, 14]  2,048                True
│    │    └─ReLU (relu)                  [128, 1024, 14, 14]  [128, 1024, 14, 14]  --                   --
├─Sequential (layer4)                    [128, 1024, 14, 14]  [128, 2048, 7, 7]    --                   True
│    └─Bottleneck (0)                    [128, 1024, 14, 14]  [128, 2048, 7, 7]    --                   True
│    │    └─Conv2d (conv1)               [128, 1024, 14, 14]  [128, 512, 14, 14]   524,288              True
│    │    └─BatchNorm2d (bn1)            [128, 512, 14, 14]   [128, 512, 14, 14]   1,024                True
│    │    └─ReLU (relu)                  [128, 512, 14, 14]   [128, 512, 14, 14]   --                   --
│    │    └─Conv2d (conv2)               [128, 512, 14, 14]   [128, 512, 7, 7]     2,359,296            True
│    │    └─BatchNorm2d (bn2)            [128, 512, 7, 7]     [128, 512, 7, 7]     1,024                True
│    │    └─ReLU (relu)                  [128, 512, 7, 7]     [128, 512, 7, 7]     --                   --
│    │    └─Conv2d (conv3)               [128, 512, 7, 7]     [128, 2048, 7, 7]    1,048,576            True
│    │    └─BatchNorm2d (bn3)            [128, 2048, 7, 7]    [128, 2048, 7, 7]    4,096                True
│    │    └─Sequential (downsample)      [128, 1024, 14, 14]  [128, 2048, 7, 7]    2,101,248            True
│    │    └─ReLU (relu)                  [128, 2048, 7, 7]    [128, 2048, 7, 7]    --                   --
│    └─Bottleneck (1)                    [128, 2048, 7, 7]    [128, 2048, 7, 7]    --                   True
│    │    └─Conv2d (conv1)               [128, 2048, 7, 7]    [128, 512, 7, 7]     1,048,576            True
│    │    └─BatchNorm2d (bn1)            [128, 512, 7, 7]     [128, 512, 7, 7]     1,024                True
│    │    └─ReLU (relu)                  [128, 512, 7, 7]     [128, 512, 7, 7]     --                   --
│    │    └─Conv2d (conv2)               [128, 512, 7, 7]     [128, 512, 7, 7]     2,359,296            True
│    │    └─BatchNorm2d (bn2)            [128, 512, 7, 7]     [128, 512, 7, 7]     1,024                True
│    │    └─ReLU (relu)                  [128, 512, 7, 7]     [128, 512, 7, 7]     --                   --
│    │    └─Conv2d (conv3)               [128, 512, 7, 7]     [128, 2048, 7, 7]    1,048,576            True
│    │    └─BatchNorm2d (bn3)            [128, 2048, 7, 7]    [128, 2048, 7, 7]    4,096                True
│    │    └─ReLU (relu)                  [128, 2048, 7, 7]    [128, 2048, 7, 7]    --                   --
│    └─Bottleneck (2)                    [128, 2048, 7, 7]    [128, 2048, 7, 7]    --                   True
│    │    └─Conv2d (conv1)               [128, 2048, 7, 7]    [128, 512, 7, 7]     1,048,576            True
│    │    └─BatchNorm2d (bn1)            [128, 512, 7, 7]     [128, 512, 7, 7]     1,024                True
│    │    └─ReLU (relu)                  [128, 512, 7, 7]     [128, 512, 7, 7]     --                   --
│    │    └─Conv2d (conv2)               [128, 512, 7, 7]     [128, 512, 7, 7]     2,359,296            True
│    │    └─BatchNorm2d (bn2)            [128, 512, 7, 7]     [128, 512, 7, 7]     1,024                True
│    │    └─ReLU (relu)                  [128, 512, 7, 7]     [128, 512, 7, 7]     --                   --
│    │    └─Conv2d (conv3)               [128, 512, 7, 7]     [128, 2048, 7, 7]    1,048,576            True
│    │    └─BatchNorm2d (bn3)            [128, 2048, 7, 7]    [128, 2048, 7, 7]    4,096                True
│    │    └─ReLU (relu)                  [128, 2048, 7, 7]    [128, 2048, 7, 7]    --                   --
├─AdaptiveAvgPool2d (avgpool)            [128, 2048, 7, 7]    [128, 2048, 1, 1]    --                   --
├─Linear (fc)                            [128, 2048]          [128, 10]            20,490               True
========================================================================================================================
Total params: 23,528,522
Trainable params: 23,528,522
Non-trainable params: 0
Total mult-adds (Units.GIGABYTES): 523.16
========================================================================================================================
Input size (MB): 77.07
Forward/backward pass size (MB): 22761.45
Params size (MB): 94.11
Estimated Total Size (MB): 22932.63
========================================================================================================================
=> no checkpoint found at 'outputs/imagenet_blurred_resnet50_0328/model_best.pth.tar'
Epoch 0/39
----------
Epoch: [0][  1/313]     Time  0.642 ( 0.642)    Data  0.092 ( 0.092)    Loss 2.3617e+00 (2.3617e+00)    Acc@1   6.25 (  6.25)   Acc@5  57.03 ( 57.03)
STAGE:2024-03-26 16:56:29 137900:18980 C:\cb\pytorch_1000000000000\work\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:314] Completed Stage: Warm Up
STAGE:2024-03-26 16:56:30 137900:18980 C:\cb\pytorch_1000000000000\work\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:320] Completed Stage: Collection
STAGE:2024-03-26 16:56:30 137900:18980 C:\cb\pytorch_1000000000000\work\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:324] Completed Stage: Post Processing
STAGE:2024-03-26 16:56:33 137900:18980 C:\cb\pytorch_1000000000000\work\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:314] Completed Stage: Warm Up
STAGE:2024-03-26 16:56:33 137900:18980 C:\cb\pytorch_1000000000000\work\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:320] Completed Stage: Collection
STAGE:2024-03-26 16:56:33 137900:18980 C:\cb\pytorch_1000000000000\work\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:324] Completed Stage: Post Processing
Epoch: [0][101/313]     Time  0.056 ( 0.126)    Data  0.025 ( 0.085)    Loss 2.2986e+00 (2.6397e+00)    Acc@1  30.47 ( 26.99)   Acc@5  83.59 ( 77.83)
Epoch: [0][201/313]     Time  0.055 ( 0.092)    Data  0.023 ( 0.055)    Loss 1.8336e+00 (2.4035e+00)    Acc@1  33.59 ( 28.04)   Acc@5  83.59 ( 80.11)
Epoch: [0][301/313]     Time  0.056 ( 0.079)    Data  0.023 ( 0.044)    Loss 1.6898e+00 (2.2428e+00)    Acc@1  38.28 ( 29.26)   Acc@5  87.50 ( 81.56)
train Loss: 2.2265 Acc: 0.2964
Epoch: [0][  1/313]     Time  0.048 ( 0.079)    Data  0.038 ( 0.043)    Loss 1.7935e+00 (2.2251e+00)    Acc@1  33.59 ( 29.65)   Acc@5  82.03 ( 81.74)
val Loss: 1.8206 Acc: 0.3397

Epoch 1/39
----------
Epoch: [1][  1/313]     Time  0.107 ( 0.107)    Data  0.074 ( 0.074)    Loss 1.5709e+00 (1.5709e+00)    Acc@1  42.97 ( 42.97)   Acc@5  93.75 ( 93.75)
Epoch: [1][101/313]     Time  0.055 ( 0.056)    Data  0.023 ( 0.024)    Loss 1.5510e+00 (1.7962e+00)    Acc@1  49.22 ( 39.69)   Acc@5  94.53 ( 89.21)
Epoch: [1][201/313]     Time  0.056 ( 0.056)    Data  0.024 ( 0.024)    Loss 1.3524e+00 (1.7110e+00)    Acc@1  57.03 ( 41.20)   Acc@5  88.28 ( 90.12)
Epoch: [1][301/313]     Time  0.081 ( 0.056)    Data  0.025 ( 0.024)    Loss 1.5540e+00 (1.6584e+00)    Acc@1  44.53 ( 42.42)   Acc@5  91.41 ( 90.65)
train Loss: 1.6529 Acc: 0.4259
Epoch: [1][  1/313]     Time  0.052 ( 0.056)    Data  0.040 ( 0.024)    Loss 3.1853e+00 (1.6578e+00)    Acc@1  52.34 ( 42.62)   Acc@5  93.75 ( 90.68)
val Loss: 5.4581 Acc: 0.4656

Epoch 2/39
----------
Epoch: [2][  1/313]     Time  0.116 ( 0.116)    Data  0.071 ( 0.071)    Loss 1.7487e+00 (1.7487e+00)    Acc@1  39.06 ( 39.06)   Acc@5  90.62 ( 90.62)
Epoch: [2][101/313]     Time  0.057 ( 0.058)    Data  0.023 ( 0.025)    Loss 1.3450e+00 (1.4682e+00)    Acc@1  55.47 ( 48.48)   Acc@5  96.88 ( 92.82)
Epoch: [2][201/313]     Time  0.054 ( 0.056)    Data  0.023 ( 0.024)    Loss 1.6175e+00 (1.4195e+00)    Acc@1  44.53 ( 50.06)   Acc@5  89.06 ( 93.21)
Epoch: [2][301/313]     Time  0.057 ( 0.056)    Data  0.024 ( 0.024)    Loss 1.4273e+00 (1.4115e+00)    Acc@1  48.44 ( 50.15)   Acc@5  92.19 ( 93.30)
train Loss: 1.4146 Acc: 0.5008
Epoch: [2][  1/313]     Time  0.048 ( 0.056)    Data  0.038 ( 0.024)    Loss 1.4106e+00 (1.4146e+00)    Acc@1  47.66 ( 50.07)   Acc@5  92.97 ( 93.28)
val Loss: 1.4813 Acc: 0.4777

Epoch 3/39
----------
Epoch: [3][  1/313]     Time  0.121 ( 0.121)    Data  0.073 ( 0.073)    Loss 1.5405e+00 (1.5405e+00)    Acc@1  42.19 ( 42.19)   Acc@5  91.41 ( 91.41)
Epoch: [3][101/313]     Time  0.056 ( 0.058)    Data  0.025 ( 0.025)    Loss 1.9074e+00 (1.3694e+00)    Acc@1  39.06 ( 51.90)   Acc@5  85.94 ( 93.37)
Epoch: [3][201/313]     Time  0.057 ( 0.057)    Data  0.024 ( 0.024)    Loss 1.6435e+00 (1.4822e+00)    Acc@1  46.88 ( 47.69)   Acc@5  90.62 ( 91.64)
Epoch: [3][301/313]     Time  0.055 ( 0.057)    Data  0.023 ( 0.024)    Loss 1.5045e+00 (1.4736e+00)    Acc@1  42.19 ( 47.88)   Acc@5  92.19 ( 91.91)
train Loss: 1.4698 Acc: 0.4798
Epoch: [3][  1/313]     Time  0.048 ( 0.057)    Data  0.036 ( 0.024)    Loss 1.3790e+00 (1.4695e+00)    Acc@1  49.22 ( 47.99)   Acc@5  91.41 ( 91.95)
val Loss: 1.3720 Acc: 0.5005

Epoch 4/39
----------
Epoch: [4][  1/313]     Time  0.120 ( 0.120)    Data  0.072 ( 0.072)    Loss 1.2990e+00 (1.2990e+00)    Acc@1  51.56 ( 51.56)   Acc@5  94.53 ( 94.53)
Epoch: [4][101/313]     Time  0.054 ( 0.060)    Data  0.023 ( 0.026)    Loss 1.2912e+00 (1.4484e+00)    Acc@1  50.00 ( 48.55)   Acc@5  96.09 ( 92.60)
Epoch: [4][201/313]     Time  0.056 ( 0.059)    Data  0.024 ( 0.025)    Loss 1.4945e+00 (1.4403e+00)    Acc@1  45.31 ( 48.63)   Acc@5  90.62 ( 92.34)
Epoch: [4][301/313]     Time  0.058 ( 0.058)    Data  0.026 ( 0.025)    Loss 1.5197e+00 (1.4856e+00)    Acc@1  46.09 ( 47.07)   Acc@5  89.84 ( 91.51)
train Loss: 1.4883 Acc: 0.4693
Epoch: [4][  1/313]     Time  0.056 ( 0.059)    Data  0.044 ( 0.025)    Loss 1.4292e+00 (1.4881e+00)    Acc@1  44.53 ( 46.92)   Acc@5  93.75 ( 91.49)
val Loss: 1.5143 Acc: 0.4464

Epoch 5/39
----------
Epoch: [5][  1/313]     Time  0.116 ( 0.116)    Data  0.074 ( 0.074)    Loss 1.3794e+00 (1.3794e+00)    Acc@1  53.91 ( 53.91)   Acc@5  91.41 ( 91.41)
Epoch: [5][101/313]     Time  0.057 ( 0.057)    Data  0.024 ( 0.024)    Loss 1.4710e+00 (1.4479e+00)    Acc@1  47.66 ( 48.13)   Acc@5  92.19 ( 91.92)
Epoch: [5][201/313]     Time  0.058 ( 0.057)    Data  0.024 ( 0.024)    Loss 1.2508e+00 (1.4216e+00)    Acc@1  53.91 ( 48.62)   Acc@5  94.53 ( 92.39)
Epoch: [5][301/313]     Time  0.064 ( 0.059)    Data  0.029 ( 0.025)    Loss 1.1679e+00 (1.3958e+00)    Acc@1  60.94 ( 49.72)   Acc@5  93.75 ( 92.63)
train Loss: 1.3914 Acc: 0.4990
Epoch: [5][  1/313]     Time  0.053 ( 0.059)    Data  0.042 ( 0.025)    Loss 1.9582e+00 (1.3932e+00)    Acc@1  53.91 ( 49.91)   Acc@5  92.19 ( 92.71)
val Loss: 1.4005 Acc: 0.5128

Epoch 6/39
----------
Epoch: [6][  1/313]     Time  0.122 ( 0.122)    Data  0.073 ( 0.073)    Loss 1.3074e+00 (1.3074e+00)    Acc@1  56.25 ( 56.25)   Acc@5  95.31 ( 95.31)
Epoch: [6][101/313]     Time  0.064 ( 0.062)    Data  0.025 ( 0.026)    Loss 1.2327e+00 (1.2489e+00)    Acc@1  50.78 ( 55.21)   Acc@5  95.31 ( 94.85)
Epoch: [6][201/313]     Time  0.061 ( 0.062)    Data  0.027 ( 0.026)    Loss 1.2367e+00 (1.2617e+00)    Acc@1  50.78 ( 55.03)   Acc@5  94.53 ( 94.42)
Epoch: [6][301/313]     Time  0.060 ( 0.062)    Data  0.026 ( 0.026)    Loss 1.2773e+00 (1.2485e+00)    Acc@1  54.69 ( 55.48)   Acc@5  93.75 ( 94.50)
train Loss: 1.2476 Acc: 0.5552
Epoch: [6][  1/313]     Time  0.051 ( 0.062)    Data  0.039 ( 0.026)    Loss 1.2500e+00 (1.2476e+00)    Acc@1  54.69 ( 55.51)   Acc@5  95.31 ( 94.52)
val Loss: 1.2814 Acc: 0.5400

Epoch 7/39
----------
Epoch: [7][  1/313]     Time  0.134 ( 0.134)    Data  0.086 ( 0.086)    Loss 1.0526e+00 (1.0526e+00)    Acc@1  58.59 ( 58.59)   Acc@5  97.66 ( 97.66)
Epoch: [7][101/313]     Time  0.063 ( 0.062)    Data  0.026 ( 0.026)    Loss 1.0660e+00 (1.1294e+00)    Acc@1  58.59 ( 59.14)   Acc@5  96.88 ( 96.06)
Epoch: [7][201/313]     Time  0.064 ( 0.062)    Data  0.025 ( 0.026)    Loss 1.2161e+00 (1.1331e+00)    Acc@1  61.72 ( 59.37)   Acc@5  95.31 ( 95.84)
Epoch: [7][301/313]     Time  0.058 ( 0.062)    Data  0.025 ( 0.026)    Loss 1.1077e+00 (1.1375e+00)    Acc@1  58.59 ( 59.42)   Acc@5  96.88 ( 95.72)
train Loss: 1.1404 Acc: 0.5931
Epoch: [7][  1/313]     Time  0.055 ( 0.062)    Data  0.044 ( 0.026)    Loss 1.2437e+00 (1.1407e+00)    Acc@1  50.00 ( 59.28)   Acc@5  94.53 ( 95.68)
val Loss: 1.2504 Acc: 0.5568

Epoch 8/39
----------
Epoch: [8][  1/313]     Time  0.124 ( 0.124)    Data  0.075 ( 0.075)    Loss 1.2773e+00 (1.2773e+00)    Acc@1  53.12 ( 53.12)   Acc@5  93.75 ( 93.75)
Epoch: [8][101/313]     Time  0.056 ( 0.062)    Data  0.023 ( 0.026)    Loss 1.3168e+00 (1.1813e+00)    Acc@1  56.25 ( 57.73)   Acc@5  95.31 ( 95.19)
Epoch: [8][201/313]     Time  0.057 ( 0.061)    Data  0.024 ( 0.026)    Loss 1.0798e+00 (1.1712e+00)    Acc@1  63.28 ( 58.33)   Acc@5  96.88 ( 95.26)
Epoch: [8][301/313]     Time  0.056 ( 0.060)    Data  0.023 ( 0.025)    Loss 1.2575e+00 (1.1535e+00)    Acc@1  53.12 ( 59.13)   Acc@5  91.41 ( 95.44)
train Loss: 1.1552 Acc: 0.5902
Epoch: [8][  1/313]     Time  0.049 ( 0.060)    Data  0.038 ( 0.025)    Loss 1.1202e+00 (1.1551e+00)    Acc@1  54.69 ( 59.01)   Acc@5  97.66 ( 95.46)
val Loss: 1.2222 Acc: 0.5643

Epoch 9/39
----------
Epoch: [9][  1/313]     Time  0.141 ( 0.141)    Data  0.091 ( 0.091)    Loss 1.0139e+00 (1.0139e+00)    Acc@1  62.50 ( 62.50)   Acc@5  96.09 ( 96.09)
Epoch: [9][101/313]     Time  0.058 ( 0.061)    Data  0.023 ( 0.025)    Loss 1.2257e+00 (1.0858e+00)    Acc@1  57.81 ( 60.96)   Acc@5  96.09 ( 96.16)
Epoch: [9][201/313]     Time  0.057 ( 0.063)    Data  0.022 ( 0.027)    Loss 8.7573e-01 (1.0563e+00)    Acc@1  68.75 ( 62.20)   Acc@5  97.66 ( 96.42)
Epoch: [9][301/313]     Time  0.053 ( 0.061)    Data  0.022 ( 0.026)    Loss 1.0546e+00 (1.0744e+00)    Acc@1  61.72 ( 61.68)   Acc@5  96.88 ( 96.22)
train Loss: 1.0725 Acc: 0.6176
Epoch: [9][  1/313]     Time  0.053 ( 0.061)    Data  0.041 ( 0.026)    Loss 1.1788e+00 (1.0728e+00)    Acc@1  56.25 ( 61.74)   Acc@5  96.09 ( 96.23)
val Loss: 1.1092 Acc: 0.6125

Epoch 10/39
----------
Epoch: [10][  1/313]    Time  0.121 ( 0.121)    Data  0.073 ( 0.073)    Loss 1.0843e+00 (1.0843e+00)    Acc@1  61.72 ( 61.72)   Acc@5  96.88 ( 96.88)
Epoch: [10][101/313]    Time  0.053 ( 0.058)    Data  0.022 ( 0.025)    Loss 9.1542e-01 (1.0099e+00)    Acc@1  67.19 ( 64.24)   Acc@5  98.44 ( 96.83)
Epoch: [10][201/313]    Time  0.054 ( 0.056)    Data  0.023 ( 0.024)    Loss 1.0752e+00 (1.0216e+00)    Acc@1  62.50 ( 63.84)   Acc@5  95.31 ( 96.54)
Epoch: [10][301/313]    Time  0.054 ( 0.057)    Data  0.023 ( 0.024)    Loss 1.0624e+00 (1.0196e+00)    Acc@1  64.84 ( 63.85)   Acc@5  98.44 ( 96.56)
train Loss: 1.0210 Acc: 0.6376
Epoch: [10][  1/313]    Time  0.046 ( 0.056)    Data  0.035 ( 0.024)    Loss 1.1596e+00 (1.0214e+00)    Acc@1  60.16 ( 63.74)   Acc@5  94.53 ( 96.57)
val Loss: 1.2350 Acc: 0.5625

Epoch 11/39
----------
Epoch: [11][  1/313]    Time  0.114 ( 0.114)    Data  0.070 ( 0.070)    Loss 9.5992e-01 (9.5992e-01)    Acc@1  67.19 ( 67.19)   Acc@5  96.09 ( 96.09)
Epoch: [11][101/313]    Time  0.054 ( 0.056)    Data  0.023 ( 0.024)    Loss 8.9314e-01 (9.7999e-01)    Acc@1  69.53 ( 65.32)   Acc@5  96.09 ( 96.78)
Epoch: [11][201/313]    Time  0.055 ( 0.056)    Data  0.024 ( 0.024)    Loss 1.0698e+00 (9.9029e-01)    Acc@1  65.62 ( 64.84)   Acc@5  94.53 ( 96.69)
Epoch: [11][301/313]    Time  0.054 ( 0.056)    Data  0.023 ( 0.024)    Loss 1.1011e+00 (9.8368e-01)    Acc@1  58.59 ( 64.97)   Acc@5  96.88 ( 96.82)
train Loss: 0.9833 Acc: 0.6500
Epoch: [11][  1/313]    Time  0.046 ( 0.056)    Data  0.035 ( 0.024)    Loss 1.2842e+00 (9.8422e-01)    Acc@1  53.12 ( 64.96)   Acc@5  94.53 ( 96.84)
val Loss: 1.0714 Acc: 0.6233

Epoch 12/39
----------
Epoch: [12][  1/313]    Time  0.233 ( 0.233)    Data  0.186 ( 0.186)    Loss 8.2351e-01 (8.2351e-01)    Acc@1  67.97 ( 67.97)   Acc@5 100.00 (100.00)
Epoch: [12][101/313]    Time  0.054 ( 0.057)    Data  0.023 ( 0.025)    Loss 8.0545e-01 (8.4992e-01)    Acc@1  69.53 ( 70.09)   Acc@5  99.22 ( 97.99)
Epoch: [12][201/313]    Time  0.058 ( 0.058)    Data  0.025 ( 0.025)    Loss 8.3924e-01 (8.5935e-01)    Acc@1  67.97 ( 69.58)   Acc@5  96.88 ( 97.92)
Epoch: [12][301/313]    Time  0.054 ( 0.057)    Data  0.023 ( 0.025)    Loss 9.8702e-01 (8.8715e-01)    Acc@1  64.84 ( 68.73)   Acc@5  97.66 ( 97.67)
train Loss: 0.8951 Acc: 0.6849
Epoch: [12][  1/313]    Time  0.047 ( 0.057)    Data  0.036 ( 0.025)    Loss 1.6454e+00 (8.9746e-01)    Acc@1  48.44 ( 68.42)   Acc@5  93.75 ( 97.58)
val Loss: 1.5635 Acc: 0.5009

Epoch 13/39
----------
Epoch: [13][  1/313]    Time  0.119 ( 0.119)    Data  0.070 ( 0.070)    Loss 1.1207e+00 (1.1207e+00)    Acc@1  58.59 ( 58.59)   Acc@5  95.31 ( 95.31)
Epoch: [13][101/313]    Time  0.057 ( 0.058)    Data  0.025 ( 0.025)    Loss 8.4423e-01 (8.8232e-01)    Acc@1  70.31 ( 68.63)   Acc@5  96.09 ( 97.82)
Epoch: [13][201/313]    Time  0.054 ( 0.057)    Data  0.023 ( 0.024)    Loss 9.3034e-01 (8.6723e-01)    Acc@1  68.75 ( 69.50)   Acc@5  96.09 ( 97.81)
Epoch: [13][301/313]    Time  0.053 ( 0.056)    Data  0.023 ( 0.024)    Loss 1.1199e+00 (8.5037e-01)    Acc@1  60.16 ( 70.06)   Acc@5  96.09 ( 97.85)
train Loss: 0.8474 Acc: 0.7016
Epoch: [13][  1/313]    Time  0.051 ( 0.056)    Data  0.035 ( 0.024)    Loss 1.1676e+00 (8.4843e-01)    Acc@1  60.94 ( 70.13)   Acc@5  96.09 ( 97.86)
val Loss: 1.0181 Acc: 0.6517

Epoch 14/39
----------
Epoch: [14][  1/313]    Time  0.116 ( 0.116)    Data  0.070 ( 0.070)    Loss 6.3468e-01 (6.3468e-01)    Acc@1  76.56 ( 76.56)   Acc@5 100.00 (100.00)
Epoch: [14][101/313]    Time  0.055 ( 0.057)    Data  0.023 ( 0.024)    Loss 6.7445e-01 (6.7754e-01)    Acc@1  78.91 ( 75.91)   Acc@5  96.09 ( 98.65)
Epoch: [14][201/313]    Time  0.054 ( 0.056)    Data  0.023 ( 0.024)    Loss 8.2209e-01 (7.2071e-01)    Acc@1  73.44 ( 74.48)   Acc@5  97.66 ( 98.48)
Epoch: [14][301/313]    Time  0.055 ( 0.056)    Data  0.024 ( 0.024)    Loss 7.1340e-01 (7.3327e-01)    Acc@1  72.66 ( 74.15)   Acc@5  99.22 ( 98.44)
train Loss: 0.7339 Acc: 0.7415
Epoch: [14][  1/313]    Time  0.047 ( 0.056)    Data  0.036 ( 0.024)    Loss 9.6879e-01 (7.3468e-01)    Acc@1  71.09 ( 74.14)   Acc@5  96.88 ( 98.43)
val Loss: 0.9902 Acc: 0.6600

Epoch 15/39
----------
Epoch: [15][  1/313]    Time  0.115 ( 0.115)    Data  0.070 ( 0.070)    Loss 6.3630e-01 (6.3630e-01)    Acc@1  77.34 ( 77.34)   Acc@5 100.00 (100.00)
Epoch: [15][101/313]    Time  0.053 ( 0.056)    Data  0.022 ( 0.024)    Loss 6.2037e-01 (6.1150e-01)    Acc@1  75.00 ( 78.57)   Acc@5 100.00 ( 99.09)
Epoch: [15][201/313]    Time  0.055 ( 0.055)    Data  0.023 ( 0.023)    Loss 6.1865e-01 (6.3979e-01)    Acc@1  82.03 ( 77.53)   Acc@5  99.22 ( 98.97)
Epoch: [15][301/313]    Time  0.053 ( 0.055)    Data  0.023 ( 0.023)    Loss 7.6519e-01 (6.5527e-01)    Acc@1  72.66 ( 76.78)   Acc@5  98.44 ( 98.90)
train Loss: 0.6557 Acc: 0.7677
Epoch: [15][  1/313]    Time  0.046 ( 0.055)    Data  0.035 ( 0.023)    Loss 9.2724e-01 (6.5660e-01)    Acc@1  65.62 ( 76.73)   Acc@5  96.88 ( 98.89)
val Loss: 0.9738 Acc: 0.6741

Epoch 16/39
----------
Epoch: [16][  1/313]    Time  0.115 ( 0.115)    Data  0.070 ( 0.070)    Loss 4.5295e-01 (4.5295e-01)    Acc@1  80.47 ( 80.47)   Acc@5 100.00 (100.00)
Epoch: [16][101/313]    Time  0.053 ( 0.056)    Data  0.021 ( 0.024)    Loss 6.0875e-01 (5.4342e-01)    Acc@1  77.34 ( 80.75)   Acc@5  98.44 ( 99.25)
Epoch: [16][201/313]    Time  0.054 ( 0.055)    Data  0.023 ( 0.024)    Loss 5.9913e-01 (5.7306e-01)    Acc@1  79.69 ( 79.57)   Acc@5  99.22 ( 99.21)
Epoch: [16][301/313]    Time  0.054 ( 0.055)    Data  0.024 ( 0.024)    Loss 6.4183e-01 (5.9076e-01)    Acc@1  70.31 ( 78.96)   Acc@5  99.22 ( 99.14)
train Loss: 0.5950 Acc: 0.7885
Epoch: [16][  1/313]    Time  0.048 ( 0.055)    Data  0.037 ( 0.024)    Loss 1.1875e+00 (5.9694e-01)    Acc@1  67.19 ( 78.81)   Acc@5  95.31 ( 99.11)
val Loss: 0.9796 Acc: 0.6839

Epoch 17/39
----------
Epoch: [17][  1/313]    Time  0.116 ( 0.116)    Data  0.070 ( 0.070)    Loss 5.0142e-01 (5.0142e-01)    Acc@1  82.03 ( 82.03)   Acc@5 100.00 (100.00)
Epoch: [17][101/313]    Time  0.055 ( 0.056)    Data  0.024 ( 0.024)    Loss 4.4469e-01 (4.7863e-01)    Acc@1  83.59 ( 82.73)   Acc@5 100.00 ( 99.50)
Epoch: [17][201/313]    Time  0.054 ( 0.056)    Data  0.023 ( 0.024)    Loss 5.3662e-01 (5.0838e-01)    Acc@1  83.59 ( 81.83)   Acc@5  99.22 ( 99.35)
Epoch: [17][301/313]    Time  0.060 ( 0.058)    Data  0.025 ( 0.025)    Loss 5.0136e-01 (5.2970e-01)    Acc@1  78.91 ( 81.05)   Acc@5 100.00 ( 99.34)
train Loss: 0.5318 Acc: 0.8099
Epoch: [17][  1/313]    Time  0.050 ( 0.058)    Data  0.038 ( 0.025)    Loss 9.1449e-01 (5.3307e-01)    Acc@1  70.31 ( 80.95)   Acc@5  97.66 ( 99.35)
val Loss: 0.9760 Acc: 0.6842

Epoch 18/39
----------
Epoch: [18][  1/313]    Time  0.122 ( 0.122)    Data  0.074 ( 0.074)    Loss 3.5219e-01 (3.5219e-01)    Acc@1  85.94 ( 85.94)   Acc@5 100.00 (100.00)
Epoch: [18][101/313]    Time  0.056 ( 0.059)    Data  0.024 ( 0.025)    Loss 4.7140e-01 (4.2456e-01)    Acc@1  80.47 ( 84.89)   Acc@5  99.22 ( 99.65)
Epoch: [18][201/313]    Time  0.056 ( 0.061)    Data  0.023 ( 0.026)    Loss 6.8679e-01 (4.6199e-01)    Acc@1  78.12 ( 83.73)   Acc@5  96.88 ( 99.56)
Epoch: [18][301/313]    Time  0.057 ( 0.063)    Data  0.024 ( 0.027)    Loss 5.6108e-01 (4.8389e-01)    Acc@1  79.69 ( 82.89)   Acc@5 100.00 ( 99.53)
train Loss: 0.4872 Acc: 0.8281
Epoch: [18][  1/313]    Time  0.051 ( 0.063)    Data  0.039 ( 0.027)    Loss 9.0829e-01 (4.8851e-01)    Acc@1  71.09 ( 82.77)   Acc@5  96.88 ( 99.51)
val Loss: 1.0250 Acc: 0.6718

Epoch 19/39
----------
Epoch: [19][  1/313]    Time  0.115 ( 0.115)    Data  0.070 ( 0.070)    Loss 3.5148e-01 (3.5148e-01)    Acc@1  88.28 ( 88.28)   Acc@5 100.00 (100.00)
Epoch: [19][101/313]    Time  0.055 ( 0.056)    Data  0.024 ( 0.024)    Loss 4.1403e-01 (3.7561e-01)    Acc@1  85.16 ( 87.00)   Acc@5  98.44 ( 99.64)
Epoch: [19][201/313]    Time  0.057 ( 0.056)    Data  0.024 ( 0.024)    Loss 3.8929e-01 (4.0935e-01)    Acc@1  86.72 ( 85.70)   Acc@5  99.22 ( 99.65)
Epoch: [19][301/313]    Time  0.055 ( 0.055)    Data  0.024 ( 0.024)    Loss 5.2025e-01 (4.3075e-01)    Acc@1  83.59 ( 84.78)   Acc@5  99.22 ( 99.58)
train Loss: 0.4335 Acc: 0.8466
Epoch: [19][  1/313]    Time  0.045 ( 0.055)    Data  0.035 ( 0.024)    Loss 1.0181e+00 (4.3541e-01)    Acc@1  67.97 ( 84.60)   Acc@5  97.66 ( 99.58)
val Loss: 1.0828 Acc: 0.6701

Epoch 20/39
----------
Epoch: [20][  1/313]    Time  0.114 ( 0.114)    Data  0.070 ( 0.070)    Loss 3.5651e-01 (3.5651e-01)    Acc@1  85.16 ( 85.16)   Acc@5 100.00 (100.00)
Epoch: [20][101/313]    Time  0.058 ( 0.056)    Data  0.024 ( 0.024)    Loss 3.2425e-01 (3.3204e-01)    Acc@1  86.72 ( 88.15)   Acc@5 100.00 ( 99.86)
Epoch: [20][201/313]    Time  0.056 ( 0.056)    Data  0.023 ( 0.024)    Loss 3.9208e-01 (3.6403e-01)    Acc@1  82.03 ( 86.88)   Acc@5 100.00 ( 99.78)
Epoch: [20][301/313]    Time  0.057 ( 0.057)    Data  0.023 ( 0.024)    Loss 5.8667e-01 (3.8382e-01)    Acc@1  77.34 ( 86.26)   Acc@5  98.44 ( 99.73)
train Loss: 0.3852 Acc: 0.8623
Epoch: [20][  1/313]    Time  0.047 ( 0.057)    Data  0.037 ( 0.024)    Loss 1.0569e+00 (3.8739e-01)    Acc@1  72.66 ( 86.18)   Acc@5  95.31 ( 99.71)
val Loss: 1.0881 Acc: 0.6878

Epoch 21/39
----------
Epoch: [21][  1/313]    Time  0.116 ( 0.116)    Data  0.071 ( 0.071)    Loss 2.5749e-01 (2.5749e-01)    Acc@1  90.62 ( 90.62)   Acc@5 100.00 (100.00)
Epoch: [21][101/313]    Time  0.067 ( 0.061)    Data  0.031 ( 0.027)    Loss 3.2161e-01 (2.9566e-01)    Acc@1  91.41 ( 89.55)   Acc@5 100.00 ( 99.88)
Epoch: [21][201/313]    Time  0.058 ( 0.059)    Data  0.027 ( 0.026)    Loss 3.0510e-01 (3.2192e-01)    Acc@1  90.62 ( 88.39)   Acc@5 100.00 ( 99.85)
Epoch: [21][301/313]    Time  0.057 ( 0.058)    Data  0.024 ( 0.025)    Loss 4.9261e-01 (3.4517e-01)    Acc@1  80.47 ( 87.58)   Acc@5 100.00 ( 99.83)
train Loss: 0.3474 Acc: 0.8753
Epoch: [21][  1/313]    Time  0.049 ( 0.058)    Data  0.036 ( 0.025)    Loss 1.1408e+00 (3.4991e-01)    Acc@1  71.09 ( 87.48)   Acc@5  96.09 ( 99.81)
val Loss: 1.0820 Acc: 0.6885

Epoch 22/39
----------
Epoch: [22][  1/313]    Time  0.116 ( 0.116)    Data  0.072 ( 0.072)    Loss 2.0328e-01 (2.0328e-01)    Acc@1  93.75 ( 93.75)   Acc@5 100.00 (100.00)
Epoch: [22][101/313]    Time  0.054 ( 0.057)    Data  0.023 ( 0.024)    Loss 2.5861e-01 (2.6890e-01)    Acc@1  89.84 ( 90.58)   Acc@5 100.00 ( 99.92)
Epoch: [22][201/313]    Time  0.054 ( 0.056)    Data  0.024 ( 0.024)    Loss 3.6788e-01 (2.9233e-01)    Acc@1  85.16 ( 89.64)   Acc@5 100.00 ( 99.90)
Epoch: [22][301/313]    Time  0.057 ( 0.055)    Data  0.024 ( 0.024)    Loss 3.6509e-01 (3.1192e-01)    Acc@1  89.06 ( 88.86)   Acc@5  99.22 ( 99.88)
train Loss: 0.3129 Acc: 0.8881
Epoch: [22][  1/313]    Time  0.046 ( 0.055)    Data  0.035 ( 0.024)    Loss 1.1885e+00 (3.1568e-01)    Acc@1  67.97 ( 88.75)   Acc@5  98.44 ( 99.88)
val Loss: 1.1808 Acc: 0.6850

Epoch 23/39
----------
Epoch: [23][  1/313]    Time  0.116 ( 0.116)    Data  0.073 ( 0.073)    Loss 2.7081e-01 (2.7081e-01)    Acc@1  88.28 ( 88.28)   Acc@5 100.00 (100.00)
Epoch: [23][101/313]    Time  0.053 ( 0.057)    Data  0.022 ( 0.024)    Loss 3.0801e-01 (2.4679e-01)    Acc@1  90.62 ( 91.47)   Acc@5 100.00 ( 99.92)
Epoch: [23][201/313]    Time  0.055 ( 0.056)    Data  0.024 ( 0.024)    Loss 1.6012e-01 (2.6644e-01)    Acc@1  96.09 ( 90.66)   Acc@5 100.00 ( 99.90)
Epoch: [23][301/313]    Time  0.054 ( 0.056)    Data  0.024 ( 0.024)    Loss 3.6823e-01 (2.7963e-01)    Acc@1  88.28 ( 90.15)   Acc@5 100.00 ( 99.88)
train Loss: 0.2806 Acc: 0.9014
Epoch: [23][  1/313]    Time  0.052 ( 0.056)    Data  0.038 ( 0.024)    Loss 1.2801e+00 (2.8380e-01)    Acc@1  68.75 ( 90.07)   Acc@5  93.75 ( 99.85)
val Loss: 1.2308 Acc: 0.6840

Epoch 24/39
----------
Epoch: [24][  1/313]    Time  0.209 ( 0.209)    Data  0.170 ( 0.170)    Loss 1.4581e-01 (1.4581e-01)    Acc@1  96.09 ( 96.09)   Acc@5 100.00 (100.00)
Epoch: [24][101/313]    Time  0.058 ( 0.077)    Data  0.026 ( 0.034)    Loss 1.8414e-01 (2.1578e-01)    Acc@1  94.53 ( 92.40)   Acc@5 100.00 ( 99.95)
Epoch: [24][201/313]    Time  0.054 ( 0.067)    Data  0.023 ( 0.029)    Loss 1.9797e-01 (2.4287e-01)    Acc@1  89.06 ( 91.46)   Acc@5 100.00 ( 99.96)
Epoch: [24][301/313]    Time  0.060 ( 0.064)    Data  0.025 ( 0.027)    Loss 2.7381e-01 (2.5331e-01)    Acc@1  89.06 ( 91.06)   Acc@5 100.00 ( 99.94)
train Loss: 0.2542 Acc: 0.9097
Epoch: [24][  1/313]    Time  0.051 ( 0.063)    Data  0.039 ( 0.027)    Loss 1.0486e+00 (2.5678e-01)    Acc@1  67.97 ( 90.90)   Acc@5  96.88 ( 99.93)
val Loss: 1.3051 Acc: 0.6713

Epoch 25/39
----------
Epoch: [25][  1/313]    Time  0.115 ( 0.115)    Data  0.071 ( 0.071)    Loss 2.6744e-01 (2.6744e-01)    Acc@1  92.19 ( 92.19)   Acc@5 100.00 (100.00)
Epoch: [25][101/313]    Time  0.053 ( 0.056)    Data  0.023 ( 0.024)    Loss 1.4275e-01 (1.9261e-01)    Acc@1  94.53 ( 93.20)   Acc@5 100.00 ( 99.98)
Epoch: [25][201/313]    Time  0.059 ( 0.056)    Data  0.025 ( 0.024)    Loss 2.8000e-01 (2.1352e-01)    Acc@1  91.41 ( 92.53)   Acc@5 100.00 ( 99.95)
Epoch: [25][301/313]    Time  0.057 ( 0.056)    Data  0.024 ( 0.024)    Loss 2.7490e-01 (2.2920e-01)    Acc@1  89.06 ( 91.94)   Acc@5 100.00 ( 99.94)
train Loss: 0.2315 Acc: 0.9184
Epoch: [25][  1/313]    Time  0.046 ( 0.056)    Data  0.035 ( 0.024)    Loss 1.5301e+00 (2.3561e-01)    Acc@1  66.41 ( 91.76)   Acc@5  94.53 ( 99.92)
val Loss: 1.2562 Acc: 0.6865

Epoch 26/39
----------
Epoch: [26][  1/313]    Time  0.120 ( 0.120)    Data  0.072 ( 0.072)    Loss 1.8989e-01 (1.8989e-01)    Acc@1  93.75 ( 93.75)   Acc@5 100.00 (100.00)
Epoch: [26][101/313]    Time  0.058 ( 0.057)    Data  0.022 ( 0.024)    Loss 1.2701e-01 (1.7501e-01)    Acc@1  96.88 ( 94.00)   Acc@5 100.00 (100.00)
Epoch: [26][201/313]    Time  0.092 ( 0.058)    Data  0.028 ( 0.025)    Loss 2.8209e-01 (1.9165e-01)    Acc@1  88.28 ( 93.37)   Acc@5 100.00 ( 99.99)
Epoch: [26][301/313]    Time  0.054 ( 0.059)    Data  0.023 ( 0.025)    Loss 1.7886e-01 (2.0997e-01)    Acc@1  92.19 ( 92.61)   Acc@5 100.00 ( 99.96)
train Loss: 0.2113 Acc: 0.9254
Epoch: [26][  1/313]    Time  0.052 ( 0.059)    Data  0.040 ( 0.025)    Loss 1.4295e+00 (2.1515e-01)    Acc@1  60.16 ( 92.44)   Acc@5  97.66 ( 99.96)
val Loss: 1.2629 Acc: 0.6854

Epoch 27/39
----------
Epoch: [27][  1/313]    Time  0.114 ( 0.114)    Data  0.072 ( 0.072)    Loss 2.4092e-01 (2.4092e-01)    Acc@1  90.62 ( 90.62)   Acc@5 100.00 (100.00)
Epoch: [27][101/313]    Time  0.059 ( 0.058)    Data  0.025 ( 0.024)    Loss 8.9580e-02 (1.7063e-01)    Acc@1  97.66 ( 94.07)   Acc@5 100.00 ( 99.98)
Epoch: [27][201/313]    Time  0.055 ( 0.057)    Data  0.025 ( 0.024)    Loss 1.8567e-01 (1.6765e-01)    Acc@1  92.97 ( 94.03)   Acc@5 100.00 ( 99.96)
Epoch: [27][301/313]    Time  0.054 ( 0.057)    Data  0.024 ( 0.024)    Loss 1.3183e-01 (1.8884e-01)    Acc@1  95.31 ( 93.24)   Acc@5 100.00 ( 99.96)
train Loss: 0.1910 Acc: 0.9317
Epoch: [27][  1/313]    Time  0.051 ( 0.057)    Data  0.039 ( 0.024)    Loss 1.2620e+00 (1.9443e-01)    Acc@1  65.62 ( 93.08)   Acc@5  96.09 ( 99.95)
val Loss: 1.3426 Acc: 0.6791

Epoch 28/39
----------
Epoch: [28][  1/313]    Time  0.139 ( 0.139)    Data  0.079 ( 0.079)    Loss 1.0861e-01 (1.0861e-01)    Acc@1  95.31 ( 95.31)   Acc@5 100.00 (100.00)
Epoch: [28][101/313]    Time  0.065 ( 0.071)    Data  0.024 ( 0.031)    Loss 1.5997e-01 (1.5829e-01)    Acc@1  94.53 ( 94.69)   Acc@5 100.00 ( 99.98)
Epoch: [28][201/313]    Time  0.059 ( 0.065)    Data  0.026 ( 0.028)    Loss 1.5119e-01 (1.6536e-01)    Acc@1  92.97 ( 94.32)   Acc@5 100.00 ( 99.98)
Epoch: [28][301/313]    Time  0.055 ( 0.062)    Data  0.024 ( 0.027)    Loss 2.6050e-01 (1.7364e-01)    Acc@1  89.06 ( 93.98)   Acc@5 100.00 ( 99.97)
train Loss: 0.1748 Acc: 0.9389
Epoch: [28][  1/313]    Time  0.049 ( 0.062)    Data  0.038 ( 0.027)    Loss 1.2068e+00 (1.7807e-01)    Acc@1  68.75 ( 93.81)   Acc@5  96.88 ( 99.96)
val Loss: 1.3097 Acc: 0.6848

Epoch 29/39
----------
Epoch: [29][  1/313]    Time  0.117 ( 0.117)    Data  0.071 ( 0.071)    Loss 2.8085e-01 (2.8085e-01)    Acc@1  89.84 ( 89.84)   Acc@5 100.00 (100.00)
Epoch: [29][101/313]    Time  0.054 ( 0.061)    Data  0.023 ( 0.026)    Loss 9.3720e-02 (1.4312e-01)    Acc@1  97.66 ( 95.20)   Acc@5 100.00 ( 99.98)
Epoch: [29][201/313]    Time  0.056 ( 0.058)    Data  0.024 ( 0.025)    Loss 9.9927e-02 (1.6081e-01)    Acc@1  97.66 ( 94.43)   Acc@5 100.00 ( 99.98)
Epoch: [29][301/313]    Time  0.060 ( 0.057)    Data  0.024 ( 0.024)    Loss 2.2604e-01 (1.7643e-01)    Acc@1  92.97 ( 93.86)   Acc@5 100.00 ( 99.97)
train Loss: 0.1783 Acc: 0.9380
Epoch: [29][  1/313]    Time  0.052 ( 0.057)    Data  0.040 ( 0.024)    Loss 1.3788e+00 (1.8208e-01)    Acc@1  73.44 ( 93.74)   Acc@5  96.09 ( 99.96)
val Loss: 1.2746 Acc: 0.6898

Epoch 30/39
----------
Epoch: [30][  1/313]    Time  0.125 ( 0.125)    Data  0.075 ( 0.075)    Loss 1.1989e-01 (1.1989e-01)    Acc@1  96.09 ( 96.09)   Acc@5 100.00 (100.00)
Epoch: [30][101/313]    Time  0.059 ( 0.059)    Data  0.024 ( 0.025)    Loss 1.0524e-01 (1.2948e-01)    Acc@1  96.09 ( 95.65)   Acc@5 100.00 ( 99.98)
Epoch: [30][201/313]    Time  0.160 ( 0.063)    Data  0.056 ( 0.027)    Loss 1.1004e-01 (1.3382e-01)    Acc@1  96.09 ( 95.38)   Acc@5 100.00 ( 99.98)
Epoch: [30][301/313]    Time  0.062 ( 0.063)    Data  0.028 ( 0.027)    Loss 2.3089e-01 (1.4900e-01)    Acc@1  91.41 ( 94.92)   Acc@5 100.00 ( 99.99)
train Loss: 0.1503 Acc: 0.9488
Epoch: [30][  1/313]    Time  0.054 ( 0.063)    Data  0.041 ( 0.027)    Loss 1.0021e+00 (1.5299e-01)    Acc@1  75.00 ( 94.81)   Acc@5  98.44 ( 99.98)
val Loss: 1.3678 Acc: 0.6895

Epoch 31/39
----------
Epoch: [31][  1/313]    Time  0.127 ( 0.127)    Data  0.080 ( 0.080)    Loss 1.7224e-01 (1.7224e-01)    Acc@1  93.75 ( 93.75)   Acc@5 100.00 (100.00)
Epoch: [31][101/313]    Time  0.054 ( 0.069)    Data  0.024 ( 0.030)    Loss 9.6570e-02 (1.2938e-01)    Acc@1  96.88 ( 95.66)   Acc@5 100.00 (100.00)
Epoch: [31][201/313]    Time  0.055 ( 0.063)    Data  0.024 ( 0.027)    Loss 1.8504e-01 (1.4495e-01)    Acc@1  91.41 ( 94.97)   Acc@5 100.00 ( 99.98)
Epoch: [31][301/313]    Time  0.056 ( 0.060)    Data  0.024 ( 0.026)    Loss 2.9106e-01 (1.5718e-01)    Acc@1  91.41 ( 94.53)   Acc@5 100.00 ( 99.98)
train Loss: 0.1574 Acc: 0.9452
Epoch: [31][  1/313]    Time  0.072 ( 0.061)    Data  0.057 ( 0.026)    Loss 1.2028e+00 (1.6076e-01)    Acc@1  72.66 ( 94.45)   Acc@5  96.09 ( 99.97)
val Loss: 1.3266 Acc: 0.6897

Epoch 32/39
----------
Epoch: [32][  1/313]    Time  0.118 ( 0.118)    Data  0.071 ( 0.071)    Loss 1.2237e-01 (1.2237e-01)    Acc@1  94.53 ( 94.53)   Acc@5 100.00 (100.00)
Epoch: [32][101/313]    Time  0.055 ( 0.057)    Data  0.024 ( 0.024)    Loss 8.9254e-02 (1.0974e-01)    Acc@1  97.66 ( 96.26)   Acc@5 100.00 ( 99.99)
Epoch: [32][201/313]    Time  0.056 ( 0.056)    Data  0.024 ( 0.024)    Loss 2.1399e-01 (1.2501e-01)    Acc@1  93.75 ( 95.78)   Acc@5 100.00 ( 99.98)
Epoch: [32][301/313]    Time  0.055 ( 0.056)    Data  0.023 ( 0.024)    Loss 1.2068e-01 (1.4032e-01)    Acc@1  95.31 ( 95.16)   Acc@5 100.00 ( 99.98)
train Loss: 0.1412 Acc: 0.9512
Epoch: [32][  1/313]    Time  0.047 ( 0.056)    Data  0.036 ( 0.024)    Loss 1.4410e+00 (1.4536e-01)    Acc@1  68.75 ( 95.04)   Acc@5  96.88 ( 99.97)
val Loss: 1.4108 Acc: 0.6888

Epoch 33/39
----------
Epoch: [33][  1/313]    Time  0.114 ( 0.114)    Data  0.070 ( 0.070)    Loss 1.1517e-01 (1.1517e-01)    Acc@1  96.09 ( 96.09)   Acc@5  99.22 ( 99.22)
Epoch: [33][101/313]    Time  0.054 ( 0.056)    Data  0.023 ( 0.024)    Loss 1.9930e-01 (1.2199e-01)    Acc@1  93.75 ( 95.75)   Acc@5 100.00 ( 99.98)
Epoch: [33][201/313]    Time  0.053 ( 0.056)    Data  0.022 ( 0.024)    Loss 8.6494e-02 (1.2377e-01)    Acc@1  97.66 ( 95.74)   Acc@5 100.00 ( 99.99)
Epoch: [33][301/313]    Time  0.057 ( 0.055)    Data  0.023 ( 0.024)    Loss 1.8854e-01 (1.3370e-01)    Acc@1  92.19 ( 95.36)   Acc@5 100.00 ( 99.98)
train Loss: 0.1348 Acc: 0.9532
Epoch: [33][  1/313]    Time  0.047 ( 0.055)    Data  0.035 ( 0.024)    Loss 1.5060e+00 (1.3917e-01)    Acc@1  70.31 ( 95.24)   Acc@5  96.09 ( 99.97)
val Loss: 1.4608 Acc: 0.6867

Epoch 34/39
----------
Epoch: [34][  1/313]    Time  0.114 ( 0.114)    Data  0.070 ( 0.070)    Loss 5.5120e-02 (5.5120e-02)    Acc@1  98.44 ( 98.44)   Acc@5 100.00 (100.00)
Epoch: [34][101/313]    Time  0.053 ( 0.056)    Data  0.022 ( 0.024)    Loss 9.8022e-02 (1.2071e-01)    Acc@1  96.88 ( 95.97)   Acc@5 100.00 ( 99.98)
Epoch: [34][201/313]    Time  0.059 ( 0.056)    Data  0.026 ( 0.024)    Loss 8.8446e-02 (1.1527e-01)    Acc@1  96.88 ( 96.05)   Acc@5 100.00 ( 99.99)
Epoch: [34][301/313]    Time  0.104 ( 0.059)    Data  0.045 ( 0.025)    Loss 6.8569e-02 (1.2516e-01)    Acc@1  96.88 ( 95.65)   Acc@5 100.00 ( 99.99)
train Loss: 0.1251 Acc: 0.9565
Epoch: [34][  1/313]    Time  0.055 ( 0.059)    Data  0.041 ( 0.025)    Loss 1.2001e+00 (1.2853e-01)    Acc@1  73.44 ( 95.58)   Acc@5  94.53 ( 99.97)
val Loss: 1.3999 Acc: 0.6944

Epoch 35/39
----------
Epoch: [35][  1/313]    Time  0.117 ( 0.117)    Data  0.073 ( 0.073)    Loss 3.4201e-02 (3.4201e-02)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [35][101/313]    Time  0.054 ( 0.061)    Data  0.023 ( 0.026)    Loss 5.1347e-02 (1.0228e-01)    Acc@1  98.44 ( 96.55)   Acc@5 100.00 (100.00)
Epoch: [35][201/313]    Time  0.060 ( 0.059)    Data  0.023 ( 0.025)    Loss 1.3278e-01 (1.1787e-01)    Acc@1  93.75 ( 96.02)   Acc@5 100.00 (100.00)
Epoch: [35][301/313]    Time  0.068 ( 0.062)    Data  0.033 ( 0.027)    Loss 9.3429e-02 (1.2534e-01)    Acc@1  96.88 ( 95.72)   Acc@5 100.00 ( 99.99)
train Loss: 0.1254 Acc: 0.9569
Epoch: [35][  1/313]    Time  0.051 ( 0.062)    Data  0.039 ( 0.027)    Loss 9.5085e-01 (1.2808e-01)    Acc@1  76.56 ( 95.63)   Acc@5  96.88 ( 99.98)
val Loss: 1.4636 Acc: 0.6853

Epoch 36/39
----------
Epoch: [36][  1/313]    Time  0.140 ( 0.140)    Data  0.085 ( 0.085)    Loss 1.0276e-01 (1.0276e-01)    Acc@1  97.66 ( 97.66)   Acc@5 100.00 (100.00)
Epoch: [36][101/313]    Time  0.059 ( 0.060)    Data  0.023 ( 0.026)    Loss 4.1069e-02 (1.0373e-01)    Acc@1  99.22 ( 96.51)   Acc@5 100.00 (100.00)
Epoch: [36][201/313]    Time  0.089 ( 0.064)    Data  0.040 ( 0.027)    Loss 1.9554e-01 (1.1728e-01)    Acc@1  92.97 ( 95.97)   Acc@5 100.00 (100.00)
Epoch: [36][301/313]    Time  0.055 ( 0.062)    Data  0.024 ( 0.027)    Loss 7.8335e-02 (1.2295e-01)    Acc@1  96.88 ( 95.70)   Acc@5 100.00 (100.00)
train Loss: 0.1237 Acc: 0.9569
Epoch: [36][  1/313]    Time  0.048 ( 0.062)    Data  0.037 ( 0.027)    Loss 1.5515e+00 (1.2821e-01)    Acc@1  67.97 ( 95.60)   Acc@5  95.31 ( 99.98)
val Loss: 1.3780 Acc: 0.6983

Epoch 37/39
----------
Epoch: [37][  1/313]    Time  0.119 ( 0.119)    Data  0.073 ( 0.073)    Loss 7.2381e-02 (7.2381e-02)    Acc@1  97.66 ( 97.66)   Acc@5 100.00 (100.00)
Epoch: [37][101/313]    Time  0.055 ( 0.057)    Data  0.024 ( 0.024)    Loss 1.4200e-01 (9.1662e-02)    Acc@1  96.09 ( 97.11)   Acc@5 100.00 ( 99.98)
Epoch: [37][201/313]    Time  0.067 ( 0.059)    Data  0.033 ( 0.026)    Loss 6.5927e-02 (9.5853e-02)    Acc@1  97.66 ( 96.85)   Acc@5 100.00 ( 99.99)
Epoch: [37][301/313]    Time  0.059 ( 0.062)    Data  0.025 ( 0.027)    Loss 1.4950e-01 (1.0587e-01)    Acc@1  94.53 ( 96.50)   Acc@5 100.00 ( 99.98)
train Loss: 0.1076 Acc: 0.9644
Epoch: [37][  1/313]    Time  0.046 ( 0.061)    Data  0.036 ( 0.027)    Loss 1.5256e+00 (1.1213e-01)    Acc@1  69.53 ( 96.35)   Acc@5  93.75 ( 99.97)
val Loss: 1.4175 Acc: 0.6921

Epoch 38/39
----------
Epoch: [38][  1/313]    Time  0.110 ( 0.110)    Data  0.076 ( 0.076)    Loss 1.4480e-01 (1.4480e-01)    Acc@1  95.31 ( 95.31)   Acc@5 100.00 (100.00)
Epoch: [38][101/313]    Time  0.056 ( 0.058)    Data  0.023 ( 0.025)    Loss 4.4823e-02 (1.0165e-01)    Acc@1  98.44 ( 96.49)   Acc@5 100.00 ( 99.99)
Epoch: [38][201/313]    Time  0.055 ( 0.058)    Data  0.024 ( 0.025)    Loss 7.0287e-02 (1.0453e-01)    Acc@1  97.66 ( 96.30)   Acc@5 100.00 ( 99.99)
Epoch: [38][301/313]    Time  0.058 ( 0.058)    Data  0.024 ( 0.024)    Loss 1.2819e-01 (1.1407e-01)    Acc@1  93.75 ( 96.03)   Acc@5 100.00 ( 99.99)
train Loss: 0.1158 Acc: 0.9597
Epoch: [38][  1/313]    Time  0.047 ( 0.057)    Data  0.037 ( 0.025)    Loss 9.2078e-01 (1.1838e-01)    Acc@1  76.56 ( 95.91)   Acc@5  96.88 ( 99.98)
val Loss: 1.4062 Acc: 0.6919

Epoch 39/39
----------
Epoch: [39][  1/313]    Time  0.116 ( 0.116)    Data  0.071 ( 0.071)    Loss 7.7328e-02 (7.7328e-02)    Acc@1  97.66 ( 97.66)   Acc@5 100.00 (100.00)
Epoch: [39][101/313]    Time  0.054 ( 0.067)    Data  0.023 ( 0.029)    Loss 8.0605e-02 (1.0186e-01)    Acc@1  97.66 ( 96.50)   Acc@5 100.00 ( 99.99)
Epoch: [39][201/313]    Time  0.058 ( 0.062)    Data  0.025 ( 0.027)    Loss 3.7539e-02 (1.0338e-01)    Acc@1  99.22 ( 96.49)   Acc@5 100.00 ( 99.99)
Epoch: [39][301/313]    Time  0.057 ( 0.061)    Data  0.025 ( 0.026)    Loss 2.2044e-01 (1.1102e-01)    Acc@1  92.97 ( 96.22)   Acc@5 100.00 ( 99.99)
train Loss: 0.1119 Acc: 0.9618
Epoch: [39][  1/313]    Time  0.050 ( 0.061)    Data  0.040 ( 0.026)    Loss 1.1622e+00 (1.1521e-01)    Acc@1  67.97 ( 96.09)   Acc@5  96.09 ( 99.98)
val Loss: 1.3926 Acc: 0.6921

Training complete in 14m 21s
Best val Acc: 0.698300
Test Loss: 0.275607

Test Accuracy of airplane: 72% (709/972)
Test Accuracy of automobile: 77% (764/985)
Test Accuracy of  bird: 62% (651/1038)
Test Accuracy of   cat: 60% (612/1007)
Test Accuracy of  deer: 69% (687/994)
Test Accuracy of   dog: 55% (558/1008)
Test Accuracy of  frog: 71% (720/1007)
Test Accuracy of horse: 73% (701/950)
Test Accuracy of  ship: 85% (867/1015)
Test Accuracy of truck: 69% (714/1024)

Test Accuracy (Overall): 69% (6983/10000)